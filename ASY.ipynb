{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{Channels of interstate risk sharing: United States 1963-1990 by Asrdubali et al}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import requests, zipfile, io # So that we can download and unzip files\\n\",\n",
    "from matplotlib2tikz import save as tikz_save\n",
    "import matplotlib\n",
    "import statsmodels.tsa.filters as filters\n",
    "import quandl as Quandl\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State income variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://apps.bea.gov/regional/zip/CA4.zip')\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "f = z.open('CA4_1969_2016__ALL_AREAS.csv')\n",
    "CA4_table = pd.read_csv(f,encoding = \"ISO-8859-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gross State Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep = np.arange(6,54)\n",
    "keep = keep.tolist()\n",
    "keep.append(1)\n",
    "data = CA4_table.iloc[:,keep]\n",
    "data = data[0:73554] #some useless columns\n",
    "data = data.drop_duplicates(['GeoName' ,'Description'])\n",
    "p = data.pivot(index = 'GeoName' , columns = 'Description')\n",
    "CA4_table1 = p.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://apps.bea.gov/regional/zip/spi.zip')\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "f = z.open('SA4_1929_2017__ALL_AREAS.csv')\n",
    "SA4_table = pd.read_csv(f,encoding = \"ISO-8859-1\", low_memory=False)\n",
    "keep = np.arange(41,94)\n",
    "keep = keep.tolist()\n",
    "keep.append(1)\n",
    "keep.append(6)\n",
    "data = SA4_table.iloc[:,keep]\n",
    "data = data[0:2080] #some useless columns\n",
    "data = data.drop_duplicates(['GeoName' ,'Description'])\n",
    "p = data.pivot(index = 'GeoName' , columns = 'Description')\n",
    "SA4_table1 = p.stack()\n",
    "f = z.open('SA35_1929_2017__ALL_AREAS.csv')\n",
    "SA35_table = pd.read_csv(f,encoding = \"ISO-8859-1\", low_memory=False)\n",
    "keep = np.arange(47,94)\n",
    "keep = keep.tolist()\n",
    "keep.append(1)\n",
    "keep.append(6)\n",
    "data = SA35_table.iloc[:,keep]\n",
    "data = data[0:2080] #some useless columns\n",
    "data = data.drop_duplicates(['GeoName' ,'Description'])\n",
    "p = data.pivot(index = 'GeoName' , columns = 'Description')\n",
    "SA35_table1 = p.stack()\n",
    "f = z.open('SA50_1948_2016__ALL_AREAS.csv')\n",
    "SA50_table = pd.read_csv(f,encoding = \"ISO-8859-1\", low_memory=False)\n",
    "keep = np.arange(28,75)\n",
    "keep = keep.tolist()\n",
    "keep.append(1)\n",
    "keep.append(6)\n",
    "data = SA50_table.iloc[:,keep]\n",
    "data = data[0:1199] #some useless columns\n",
    "data = data.drop_duplicates(['GeoName' ,'Description'])\n",
    "p = data.pivot(index = 'GeoName' , columns = 'Description')\n",
    "SA50_table1 = p.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GDP_deflator_annual = Quandl.get(\"FRED/GDPDEF\", authtoken=\"5QphWABG_zpJsB5dy4yW\", collapse=\"annual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Construction of GSP: changes time period from 1969, and employment instead of population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://apps.bea.gov/regional/zip/gsp/gsp_sic_all.zip')\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "f = z.open('gsp_sic_all.csv')\n",
    "GSP_sic_table = pd.read_csv(f,encoding = \"ISO-8859-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://apps.bea.gov/regional/zip/gsp/gsp_naics_all.zip')\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "f = z.open('gsp_naics_all.csv')\n",
    "GSP_naics_table = pd.read_csv(f,encoding = \"ISO-8859-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Industry_Total_GSP = GSP_sic_table.iloc[::78]\n",
    "Industry_Total_GSP = Industry_Total_GSP.iloc[1:52]\n",
    "keep = list(range(8,43))\n",
    "keep.append(1)\n",
    "Industry_Total_GSP = Industry_Total_GSP.iloc[:,keep].set_index('GeoName')\n",
    "Industry_Total_GSP = Industry_Total_GSP.applymap(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Industry_Total_GSP_naics = GSP_naics_table.iloc[::90]\n",
    "Industry_Total_GSP_naics = Industry_Total_GSP_naics.iloc[1:52]\n",
    "keep = list(range(8,28))\n",
    "keep.append(1)\n",
    "Industry_Total_GSP_naics = Industry_Total_GSP_naics.iloc[:,keep].set_index('GeoName')\n",
    "Industry_Total_GSP_naics = Industry_Total_GSP_naics.applymap(float)\n",
    "naics_factor = Industry_Total_GSP_naics.values[:,0]/Industry_Total_GSP.values[:,-1]\n",
    "Industry_Total_GSP_naics = Industry_Total_GSP_naics.drop(columns=['1997'])\n",
    "Industry_Total_GSP  = (Industry_Total_GSP * np.tile(naics_factor,(35,1)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nominal_Industry_Total_GSP = pd.concat([Industry_Total_GSP, Industry_Total_GSP_naics], axis=1)\n",
    "nominal_Industry_Total_GSP1 = nominal_Industry_Total_GSP.iloc[:,-48:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_Industry_Total_GSP = nominal_Industry_Total_GSP / GDP_deflator_annual.iloc[16:70,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = Industry_Total_GSP.index\n",
    "names_vector2 = [s + ' state total' for s in names] + [s + ' state total*' for s in names]\n",
    "CA4_table3 = CA4_table1.loc[names_vector2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Employment = CA4_table3.iloc[20::22]\n",
    "Employment = Employment.applymap(float)\n",
    "Employment_share = Employment /Employment.values.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Real_Output_pc = 100* 1000* 1000*real_Industry_Total_GSP.iloc[:,6:-1]/Employment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Population = CA4_table3.iloc[17::22]\n",
    "Population = Population.applymap(float)\n",
    "Population_share = Population /Population.values.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load federal government finances on the fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://www.whitehouse.gov/sites/whitehouse.gov/files/omb/budget/fy2018/hist.zip')\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "f = z.open('hist02z1.xls')\n",
    "FED_govt_table_2_1 = pd.read_excel(f,header = [2,3], encoding = \"ISO-8859-1\") # in millions\n",
    "FED_govt_table_2_1 = FED_govt_table_2_1.transpose()\n",
    "FED_govt_table_2_1 = FED_govt_table_2_1.drop(columns = ['TQ'])\n",
    "f = z.open('hist02z4.xls')\n",
    "FED_govt_table_2_4 = pd.read_excel(f,header = 2, encoding = \"ISO-8859-1\")\n",
    "FED_govt_table_2_4 = FED_govt_table_2_4.drop(columns = ['TQ'])\n",
    "f = z.open('hist02z5.xls')\n",
    "FED_govt_table_2_5 = pd.read_excel(f,header = [2,3], encoding = \"ISO-8859-1\") # in millions\n",
    "FED_govt_table_2_5 = FED_govt_table_2_5.transpose()\n",
    "FED_govt_table_2_5 = FED_govt_table_2_5.drop(columns = ['TQ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed state government finances -  for now downloaded as it is mostly in access database prior to 1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#r = requests.get('https://www2.census.gov/pub/outgoing/govs/special60/State_Govt_Fin.zip')\n",
    "#z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "#f = z.open('State_Govt_Finances.mdb')\n",
    "#accumulator = []\n",
    "# for now just load the excel files from the .mdb\n",
    "#for chunk in mdb.read_table(f, '1_Revenues', chunksize=10000):\n",
    "#    accumulator.append(f(chunk))\n",
    "# df = mdb.read_table('State_Govt_Finances.mdb', '1_Revenues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "State_Revenues = pd.read_excel('State_Revenues.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it from 2008 on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_Finance_08 = pd.read_fwf('https://www2.census.gov/programs-surveys/state/datasets/2008/historical-datasets/08state35.txt',header = None)\n",
    "State_Finance_09 = pd.read_fwf('https://www2.census.gov/programs-surveys/state/datasets/2009/historical-datasets/09state35.txt',header = None)\n",
    "State_Finance_10 = pd.read_fwf('https://www2.census.gov/programs-surveys/state/datasets/2010/historical-datasets/10state35.txt',header = None)\n",
    "State_Finance_11 = pd.read_fwf('https://www2.census.gov/programs-surveys/state/datasets/2011/historical-datasets/11state35.txt',header = None)\n",
    "State_Finance_12 = pd.read_fwf('https://www2.census.gov/programs-surveys/state/datasets/2012/historical-datasets/12state35.txt',header = None)\n",
    "State_Finance_13 = pd.read_fwf('https://www2.census.gov/programs-surveys/state/datasets/2013/historical-datasets/13state35.txt',header = None)\n",
    "State_Finance_14 = pd.read_fwf('https://www2.census.gov/programs-surveys/state/datasets/2014/historical-datasets/14state35.txt',header = None)\n",
    "State_Finance_15 = pd.read_fwf('https://www2.census.gov/programs-surveys/state/datasets/2015/historical-datasets/15state35.txt',header = None)\n",
    "\n",
    "State_ID =pd.read_excel('https://www2.census.gov/programs-surveys/state/technical-documentation/code-list/government-ids.xls')\n",
    "Itemcodes =pd.read_excel('https://www2.census.gov/programs-surveys/state/technical-documentation/code-list/itemcodes.xls')\n",
    "Itemcodes_old =pd.read_excel('https://www2.census.gov/programs-surveys/state/technical-documentation/code-list/itemcodes-old.xls')\n",
    "#Donwload the correct abbreviations:\n",
    "State_abbrev =pd.read_excel('http://www.downloadexcelfiles.com/sites/default/files/docs/list_of_u.s._state_abbreviations-1514j.xlsx',header = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of federal nonpersonal taxes and contributions: weights for each state is half personal income, half capital income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_corporate_income_tax = FED_govt_table_2_1.iloc[1,35:82] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corp_tax = Employment.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Personal_nominal_income = CA4_table3.iloc[13::22]\n",
    "Personal_nominal_income = Personal_nominal_income.applymap(float)\n",
    "Personal_nominal_income_share = Personal_nominal_income /Personal_nominal_income.values.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Capital_nominal_income = CA4_table3.iloc[15::22]\n",
    "Capital_nominal_income = Capital_nominal_income.applymap(float)\n",
    "Capital_nominal_income_share = Capital_nominal_income /Capital_nominal_income.values.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_corp_tax = (Capital_nominal_income_share.values + Personal_nominal_income_share.values)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corp_tax.iloc[:,:] =  (weights_corp_tax * np.tile(total_corporate_income_tax.T,(51,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#corp_tax = corp_tax  /Population.values \n",
    "#corp_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#corp_tax = corp_tax  /GDP_deflator_annual.iloc[22:69,0].values * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of tobacco/excise tax: weights for each state is half personal income, half pop_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alcohol_tax = FED_govt_table_2_4.iloc[23,29:76]\n",
    "estate_gift_tax = FED_govt_table_2_5.iloc[1,29:76]\n",
    "customs_tax = FED_govt_table_2_5.iloc[2,29:76]\n",
    "misc_tax = FED_govt_table_2_5.iloc[5,29:76]\n",
    "highway_tax = FED_govt_table_2_4.iloc[36,29:76]\n",
    "airway_tax = FED_govt_table_2_4.iloc[37,29:76]\n",
    "airway_tax = airway_tax.replace(to_replace='..........', value=0)\n",
    "crude_oil_tax = FED_govt_table_2_4.iloc[25,29:76]\n",
    "crude_oil_tax = crude_oil_tax.replace(to_replace='..........', value=0)\n",
    "telephone_tax = FED_govt_table_2_4.iloc[26,29:76]\n",
    "telephone_tax = telephone_tax.replace(to_replace='..........', value=0)\n",
    "black_lung_tax = FED_govt_table_2_4.iloc[38,29:76]\n",
    "black_lung_tax = black_lung_tax.replace(to_replace='..........', value=0)\n",
    "hazard_tax = FED_govt_table_2_4.iloc[40,29:76]\n",
    "hazard_tax = hazard_tax.replace(to_replace='..........', value=0)\n",
    "total_excise_tax = (alcohol_tax + estate_gift_tax + customs_tax + misc_tax + highway_tax + airway_tax + crude_oil_tax +\n",
    "                   telephone_tax + black_lung_tax + hazard_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_tobacco_tax = FED_govt_table_2_4.iloc[24,29:76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excise_tax = Employment.copy()\n",
    "weights_excise_tax = Personal_nominal_income_share.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excise_tax.iloc[:,:] = (weights_excise_tax * np.tile(total_excise_tax.T,(51,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#excise_tax = excise_tax  /Employment.values \n",
    "#excise_tax = excise_tax  /GDP_deflator_annual.iloc[22:69,0].values * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tobacco_tax = Employment.copy()\n",
    "weights_tobacco_tax = (Personal_nominal_income_share.values + Population_share.values)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tobacco_tax.iloc[:,:] = (weights_tobacco_tax * np.tile(total_tobacco_tax.T,(51,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of social security contributions: weights for each state is half personal income, half SS contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "federal_employment_contr = FED_govt_table_2_4.iloc[11,29:76]\n",
    "federal_UI_contr = FED_govt_table_2_4.iloc[15,29:76]\n",
    "federal_other_reti_contr = FED_govt_table_2_4.iloc[19,29:76]\n",
    "aggr_ss_contr = federal_employment_contr + federal_UI_contr + federal_other_reti_contr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "personal_social_security_contributions = CA4_table3.iloc[0::22]\n",
    "personal_social_security_contributions = personal_social_security_contributions.applymap(float)\n",
    "personal_social_security_contributions_share = personal_social_security_contributions /personal_social_security_contributions.values.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_ss_contr = (Personal_nominal_income_share.values + personal_social_security_contributions_share)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss_contr =  Employment.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss_contr = weights_ss_contr * np.tile(aggr_ss_contr.T,(51,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of state unemployment contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Total_UI_rev = State_Revenues.iloc[:,-13]\n",
    "interest_UI_rev = State_Revenues.iloc[:,-11]\n",
    "UI_tax = Total_UI_rev - interest_UI_rev\n",
    "UI_tax_df = State_Revenues.copy()\n",
    "UI_tax_df = UI_tax_df.iloc[:,[2,7,-13]]\n",
    "UI_tax_df.rename(columns={'Total Unemp Rev': 'UI_tax'}, inplace=True)\n",
    "UI_tax_df['UI_tax'] = UI_tax\n",
    "UI_tax_df = UI_tax_df.pivot(index = 'Name' , columns = 'Year4')\n",
    "UI_tax_df.index.values[9] = 'DC STATE GOVT'\n",
    "index_name = UI_tax_df.index.copy()\n",
    "index_name = np.delete(index_name, [1,13,46], None)\n",
    "UI_tax_df = UI_tax_df.reindex(index_name)\n",
    "UI_tax_df.index.values[:] = UI_tax_df.index.astype(str).str[0:2]\n",
    "UI_tax_df = UI_tax_df.reindex(State_abbrev.iloc[3:54,3].values)\n",
    "# Set DC values to 0 after 2008\n",
    "UI_tax_df.iloc[8,71] = 0\n",
    "# 2009\n",
    "State_Finance_09['State'] = State_Finance_09[0].astype(str).str[0:14]\n",
    "State_Finance_09['Item'] = State_Finance_09[0].astype(str).str[14:]\n",
    "State_Finance_09['Dollar Amount'] = State_Finance_09[1].astype(str).str[:-6]\n",
    "State_Finance_09 = State_Finance_09.drop(columns = [0,1])\n",
    "State_Finance_09_pivot = State_Finance_09.pivot(index = 'State' , columns = 'Item')\n",
    "State_Finance_09_pivot.index.values[1:] = State_ID['State']\n",
    "UI_09_Y01 = np.array(list(map(int, State_Finance_09_pivot['Dollar Amount']['Y01'].values)))\n",
    "State_Finance_09_pivot['Dollar Amount']['Y04'].values[:] = State_Finance_09_pivot['Dollar Amount']['Y04'].fillna(value=0)\n",
    "UI_09_Y04 = np.array(list(map(int, State_Finance_09_pivot['Dollar Amount']['Y04'].values)))\n",
    "UI_09 = UI_09_Y01 + UI_09_Y04\n",
    "UI_09_aug = np.zeros((51))\n",
    "UI_09_aug[:8] = UI_09[1:9]\n",
    "UI_09_aug[9:] = UI_09[9:]\n",
    "UI_tax_df['UI_tax',2009] = UI_09_aug\n",
    "# 2010\n",
    "State_Finance_10['State'] = State_Finance_10[0].astype(str).str[0:14]\n",
    "State_Finance_10['Item'] = State_Finance_10[0].astype(str).str[14:]\n",
    "State_Finance_10['Dollar Amount'] = State_Finance_10[1].astype(str).str[:-6]\n",
    "State_Finance_10 = State_Finance_10.drop(columns = [0,1])\n",
    "State_Finance_10_pivot = State_Finance_10.pivot(index = 'State' , columns = 'Item')\n",
    "State_Finance_10_pivot.index.values[1:] = State_ID['State']\n",
    "UI_10_Y01 = np.array(list(map(int, State_Finance_10_pivot['Dollar Amount']['Y01'].values)))\n",
    "State_Finance_10_pivot['Dollar Amount']['Y04'].values[:] = State_Finance_10_pivot['Dollar Amount']['Y04'].fillna(value=0)\n",
    "UI_10_Y04 = np.array(list(map(int, State_Finance_10_pivot['Dollar Amount']['Y04'].values)))\n",
    "UI_10 = UI_10_Y01 + UI_10_Y04\n",
    "UI_10_aug = np.zeros((51))\n",
    "UI_10_aug[:8] = UI_10[1:9]\n",
    "UI_10_aug[9:] = UI_10[9:]\n",
    "UI_tax_df['UI_tax',2010] = UI_10_aug\n",
    "# 2011\n",
    "State_Finance_11['State'] = State_Finance_11[0].astype(str).str[0:14]\n",
    "State_Finance_11['Item'] = State_Finance_11[0].astype(str).str[14:]\n",
    "State_Finance_11['Dollar Amount'] = State_Finance_11[1].astype(str).str[:-6]\n",
    "State_Finance_11 = State_Finance_11.drop(columns = [0,1])\n",
    "State_Finance_11_pivot = State_Finance_11.pivot(index = 'State' , columns = 'Item')\n",
    "State_Finance_11_pivot.index.values[1:] = State_ID['State']\n",
    "UI_11_Y01 = np.array(list(map(int, State_Finance_11_pivot['Dollar Amount']['Y01'].values)))\n",
    "State_Finance_11_pivot['Dollar Amount']['Y04'].values[:] = State_Finance_11_pivot['Dollar Amount']['Y04'].fillna(value=0)\n",
    "UI_11_Y04 = np.array(list(map(int, State_Finance_11_pivot['Dollar Amount']['Y04'].values)))\n",
    "UI_11 = UI_11_Y01 + UI_11_Y04\n",
    "UI_11_aug = np.zeros((51))\n",
    "UI_11_aug[:8] = UI_11[1:9]\n",
    "UI_11_aug[9:] = UI_11[9:]\n",
    "UI_tax_df['UI_tax',2011] = UI_11_aug\n",
    "# 2012\n",
    "State_Finance_12['State'] = State_Finance_12[0].astype(str).str[0:14]\n",
    "State_Finance_12['Item'] = State_Finance_12[0].astype(str).str[14:]\n",
    "State_Finance_12['Dollar Amount'] = State_Finance_12[1].astype(str).str[:-6]\n",
    "State_Finance_12 = State_Finance_12.drop(columns = [0,1])\n",
    "State_Finance_12_pivot = State_Finance_12.pivot(index = 'State' , columns = 'Item')\n",
    "State_Finance_12_pivot.index.values[1:] = State_ID['State']\n",
    "UI_12_Y01 = np.array(list(map(int, State_Finance_12_pivot['Dollar Amount']['Y01'].values)))\n",
    "State_Finance_12_pivot['Dollar Amount']['Y04'].values[:] = State_Finance_12_pivot['Dollar Amount']['Y04'].fillna(value=0)\n",
    "UI_12_Y04 = np.array(list(map(int, State_Finance_12_pivot['Dollar Amount']['Y04'].values)))\n",
    "UI_12 = UI_12_Y01 + UI_12_Y04\n",
    "UI_12_aug = np.zeros((51))\n",
    "UI_12_aug[:8] = UI_12[1:9]\n",
    "UI_12_aug[9:] = UI_12[9:]\n",
    "UI_tax_df['UI_tax',2012] = UI_12_aug\n",
    "# 2013\n",
    "State_Finance_13['State'] = State_Finance_13[0].astype(str).str[0:14]\n",
    "State_Finance_13['Item'] = State_Finance_13[0].astype(str).str[14:]\n",
    "State_Finance_13['Dollar Amount'] = State_Finance_13[1].astype(str).str[:-6]\n",
    "State_Finance_13 = State_Finance_13.drop(columns = [0,1])\n",
    "#State_Finance_13_pivot = State_Finance_13.pivot(index = 'State' , columns = 'Item')\n",
    "State_Finance_13_pivot = State_Finance_13.groupby(['State','Item']).first().unstack()\n",
    "State_Finance_13_pivot.index.values[1:] = State_ID['State']\n",
    "UI_13_Y01 = np.array(list(map(float, State_Finance_13_pivot['Dollar Amount']['Y01'].values)))\n",
    "State_Finance_13_pivot['Dollar Amount']['Y04'].values[:] = State_Finance_13_pivot['Dollar Amount']['Y04'].fillna(value=0)\n",
    "UI_13_Y04 = np.array(list(map(float, State_Finance_13_pivot['Dollar Amount']['Y04'].values)))\n",
    "UI_13 = UI_13_Y01 + UI_13_Y04\n",
    "UI_13_aug = np.zeros((51))\n",
    "UI_13_aug[:8] = UI_13[1:9]\n",
    "UI_13_aug[9:] = UI_13[9:]\n",
    "UI_tax_df['UI_tax',2013] = UI_13_aug\n",
    "# 2014\n",
    "State_Finance_14['State'] = State_Finance_14[0].astype(str).str[0:14]\n",
    "State_Finance_14['Item'] = State_Finance_14[0].astype(str).str[14:]\n",
    "State_Finance_14['Dollar Amount'] = State_Finance_14[1].astype(str).str[:-6]\n",
    "State_Finance_14 = State_Finance_14.drop(columns = [0,1])\n",
    "State_Finance_14['Dollar Amount'].replace('',0, inplace=True)\n",
    "#State_Finance_14_pivot = State_Finance_14.pivot(index = 'State' , columns = 'Item')\n",
    "State_Finance_14_pivot = (State_Finance_14.groupby(['State','Item'])\n",
    "   .first()\n",
    "   .unstack()\n",
    ")\n",
    "State_Finance_14_pivot.index.values[1:] = State_ID['State']\n",
    "UI_14_Y01 = np.array(list(map(float, State_Finance_14_pivot['Dollar Amount']['Y01'].values)))\n",
    "State_Finance_14_pivot['Dollar Amount']['Y04'].values[:] = State_Finance_14_pivot['Dollar Amount']['Y04'].replace('', 0, regex=True)\n",
    "UI_14_Y04 = np.array(list(map(float, State_Finance_14_pivot['Dollar Amount']['Y04'].values)))\n",
    "UI_14 = UI_14_Y01 + UI_14_Y04\n",
    "UI_14_aug = np.zeros((51))\n",
    "UI_14_aug[:8] = UI_14[1:9]\n",
    "UI_14_aug[9:] = UI_14[9:]\n",
    "UI_tax_df['UI_tax',2014] = UI_14_aug\n",
    "# 2015\n",
    "State_Finance_15['State'] = State_Finance_15[0].astype(str).str[0:14]\n",
    "State_Finance_15['Item'] = State_Finance_15[0].astype(str).str[14:]\n",
    "State_Finance_15['Dollar Amount'] = State_Finance_15[1].astype(str).str[:-6]\n",
    "State_Finance_15 = State_Finance_15.drop(columns = [0,1])\n",
    "#State_Finance_15_pivot = State_Finance_15.pivot(index = 'State' , columns = 'Item')\n",
    "State_Finance_15_pivot = (State_Finance_15.groupby(['State','Item'])\n",
    "   .first()\n",
    "   .unstack()\n",
    ")\n",
    "State_Finance_15_pivot.index.values[1:9] = State_ID['State'][:8]\n",
    "State_Finance_15_pivot.index.values[9:] = State_ID['State'][8:]\n",
    "State_Finance_15_pivot.drop(State_Finance_15_pivot.index[9], inplace=True)\n",
    "UI_15_Y01 = np.array(list(map(float, State_Finance_15_pivot['Dollar Amount']['Y01'].values)))\n",
    "State_Finance_15_pivot['Dollar Amount']['Y04'].values[:] = State_Finance_15_pivot['Dollar Amount']['Y04'].replace('', 0, regex=True)\n",
    "UI_15_Y04 = np.array(list(map(float, State_Finance_15_pivot['Dollar Amount']['Y04'].values)))\n",
    "UI_15 = UI_15_Y01 + UI_15_Y04\n",
    "UI_15_aug = np.zeros((51))\n",
    "UI_15_aug[:8] = UI_15[1:9]\n",
    "UI_15_aug[10:] = UI_15[9:]\n",
    "UI_tax_df['UI_tax',2015] = UI_15_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "federal_non_personal_taxes = corp_tax + tobacco_tax.values + ss_contr.values + excise_tax.values + UI_tax_df.iloc[:,32:].values/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{Construction of state and local non-personal taxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloaded from https://www2.census.gov/pub/outgoing/govs/special60/Govt_Finances.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Govt_Revenues = pd.read_excel('Govt_Revenues.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "State_local_tax = Govt_Revenues.iloc[:,19]\n",
    "State_local_tax_df = Govt_Revenues.copy()\n",
    "State_local_tax_df = State_local_tax_df.iloc[:,[2,6,19]]\n",
    "State_local_tax_df.rename(columns={'Total Taxes': 'State & local tax'}, inplace=True)\n",
    "State_local_tax_df1 = State_local_tax_df[State_local_tax_df['Name'].str.contains('STATE-LOCAL TOTAL')==True]\n",
    "State_local_tax_df_pivot = State_local_tax_df1.pivot(index = 'Name' , columns = 'Year4')\n",
    "index_name = State_local_tax_df_pivot.index.copy()\n",
    "index_name = np.delete(index_name, [1,13,46], None)\n",
    "State_local_tax_df_pivot = State_local_tax_df_pivot.reindex(index_name)\n",
    "State_local_tax_df_pivot.index.values[:] = State_local_tax_df_pivot.index.astype(str).str[0:2]\n",
    "State_local_tax_df_pivot = State_local_tax_df_pivot.reindex(State_abbrev.iloc[3:54,3].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correct for the missing data: 2001 & 2003 - total local tax distributed across state as in the previous year\n",
    "US_total_local_tax = State_local_tax_df[State_local_tax_df['Name'].str.contains('US LOCAL TOTAL')==True]\n",
    "Local_tax_df = State_local_tax_df[State_local_tax_df['Name'].str.contains('LOCAL TOTAL')==True]\n",
    "Local_tax_df = Local_tax_df[Local_tax_df['Name'].str.contains('STATE')==False]\n",
    "US_total_local_tax_2001 = US_total_local_tax.iloc[7,2]\n",
    "US_total_local_tax_2003 = US_total_local_tax.iloc[5,2]\n",
    "Local_tax_df_pivot = Local_tax_df.pivot(index = 'Name' , columns = 'Year4')\n",
    "index_name = Local_tax_df_pivot.index.copy()\n",
    "index_name = np.delete(index_name, [44], None)\n",
    "Local_tax_df_pivot = Local_tax_df_pivot.reindex(index_name)\n",
    "Local_tax_df_pivot.index.values[:] = Local_tax_df_pivot.index.astype(str).str[0:2]\n",
    "Local_tax_df_pivot = Local_tax_df_pivot.reindex(State_abbrev.iloc[3:54,3].values)\n",
    "Local_tax_df_pivot['State & local tax'][2001].values[:] = Local_tax_df_pivot['State & local tax'][2000].values/Local_tax_df_pivot['State & local tax'][2000].values.sum() * US_total_local_tax_2001\n",
    "Local_tax_df_pivot['State & local tax'][2003].values[:] = Local_tax_df_pivot['State & local tax'][2002].values/Local_tax_df_pivot['State & local tax'][2002].values.sum() * US_total_local_tax_2003\n",
    "State_tax_df = State_local_tax_df[State_local_tax_df['Name'].str.contains('STATE GOVT')==True]\n",
    "State_tax_df = State_tax_df[State_tax_df['Name'].str.contains('US')==False]\n",
    "State_tax_df_pivot = State_tax_df.pivot(index = 'Name' , columns = 'Year4')\n",
    "State_tax_df_pivot.index.values[:] = State_tax_df_pivot.index.astype(str).str[0:2]\n",
    "State_tax_df_pivot = State_tax_df_pivot.reindex(State_abbrev.iloc[3:54,3].values)\n",
    "State_local_tax_df_pivot['State & local tax'][2003].values[:] =  Local_tax_df_pivot['State & local tax'][2003].values[:] + State_tax_df_pivot['State & local tax'][2003].values[:]\n",
    "State_local_tax_df_pivot['State & local tax'][2001].values[:] =  Local_tax_df_pivot['State & local tax'][2001].values[:] + State_tax_df_pivot['State & local tax'][2001].values[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data from 2009-2015\n",
    "r_09 =  pd.read_excel('http://www2.census.gov/govs/local/09slsstab1a.xls')\n",
    "l_09 =  pd.read_excel('http://www2.census.gov/govs/local/09slsstab1b.xls')\n",
    "State_Local_Finance_09 = pd.concat([r_09, l_09], axis=1)\n",
    "State_local_tax_df_pivot['State & local tax',2009] =pd.concat([State_Local_Finance_09.iloc[22,6:130:5],State_Local_Finance_09.iloc[22,132::5]] , axis=0).values[:]\n",
    "r_10 =  pd.read_excel('http://www2.census.gov/govs/local/10slsstab1a.xls')\n",
    "l_10 =  pd.read_excel('http://www2.census.gov/govs/local/10slsstab1b.xls')\n",
    "State_Local_Finance_10 = pd.concat([r_10, l_10], axis=1)\n",
    "State_local_tax_df_pivot['State & local tax',2010] =pd.concat([State_Local_Finance_10.iloc[22,6:130:5],State_Local_Finance_10.iloc[22,132::5]] , axis=0).values[:]\n",
    "r_11 =  pd.read_excel('http://www2.census.gov/govs/local/11slsstab1a.xls')\n",
    "l_11 =  pd.read_excel('http://www2.census.gov/govs/local/11slsstab1b.xls')\n",
    "State_Local_Finance_11 = pd.concat([r_11, l_11], axis=1)\n",
    "State_local_tax_df_pivot['State & local tax',2011] =pd.concat([State_Local_Finance_11.iloc[22,6:130:5],State_Local_Finance_11.iloc[22,132::5]] , axis=0).values[:]\n",
    "r_12 =  pd.read_excel('http://www2.census.gov/govs/local/12slsstab1a.xls',header = None)\n",
    "l_12 =  pd.read_excel('http://www2.census.gov/govs/local/12slsstab1b.xls',header = None)\n",
    "State_Local_Finance_12 = pd.concat([r_12, l_12], axis=1)\n",
    "State_local_tax_df_pivot['State & local tax',2012] =pd.concat([State_Local_Finance_12.iloc[22,5:80:3],State_Local_Finance_12.iloc[22,82::3]] , axis=0).values[:]\n",
    "r_13 =  pd.read_excel('http://www2.census.gov/govs/local/13slsstab1a.xls',header = None)\n",
    "l_13 =  pd.read_excel('http://www2.census.gov/govs/local/13slsstab1b.xls',header = None)\n",
    "State_Local_Finance_13 = pd.concat([r_13, l_13], axis=1)\n",
    "State_local_tax_df_pivot['State & local tax',2013] =pd.concat([State_Local_Finance_13.iloc[24,7:130:5],State_Local_Finance_13.iloc[24,134::5]] , axis=0).values[:]\n",
    "r_14 =  pd.read_excel('http://www2.census.gov/govs/local/14slsstab1a.xls',header = None)\n",
    "l_14 =  pd.read_excel('http://www2.census.gov/govs/local/14slsstab1b.xls',header = None)\n",
    "State_Local_Finance_14 = pd.concat([r_14, l_14], axis=1)\n",
    "State_local_tax_df_pivot['State & local tax',2014] =pd.concat([State_Local_Finance_14.iloc[24,7:130:5],State_Local_Finance_14.iloc[24,134::5]] , axis=0).values[:]\n",
    "r_15 =  pd.read_excel('http://www2.census.gov/govs/local/15slsstab1a.xlsx',header = None)\n",
    "l_15 =  pd.read_excel('http://www2.census.gov/govs/local/15slsstab1b.xlsx',header = None)\n",
    "State_Local_Finance_15 = pd.concat([r_15, l_15], axis=1)\n",
    "State_local_tax_df_pivot['State & local tax',2015] =pd.concat([State_Local_Finance_15.iloc[25,7:130:5],State_Local_Finance_15.iloc[25,134::5]] , axis=0).values[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use State Personal Income accounts to contruct personal taxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_vector2 = [s for s in names] + [s + '*' for s in names]\n",
    "SA50_table3 = SA50_table1.loc[names_vector2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Personal_tax_nontax_payments_state = SA50_table3[11::18]\n",
    "Personal_tax_nontax_payments_local = SA50_table3[9::18]\n",
    "State_local_personal_property_taxes = SA50_table3[10::18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Personal_tax_nontax_payments_local_state = (Personal_tax_nontax_payments_state + Personal_tax_nontax_payments_local.values + \n",
    "                                            State_local_personal_property_taxes.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_local_non_personal_taxes = (State_local_tax_df_pivot.iloc[:,-47:] - Personal_tax_nontax_payments_local_state.values)/ 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interest on state and local funds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only state financing data is used, no local governments. This omission is because I couldnt find a reliable source for this data\n",
    "(yet). Anyway, the local part seems to be the less important part of these interest earning (comparing it to the original paper).  Simply subtract Y02 before 2008 and not add before 2008.\n",
    "Update 1 - trust interest is now both local and state added before 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interest_state_insurance_df = State_Revenues.copy()\n",
    "interest_state_insurance_df = interest_state_insurance_df.iloc[:,[2,7,156,159,164]]\n",
    "interest_state_insurance_df.iloc[:,2].values[:] = (interest_state_insurance_df.iloc[:,2].values + # Y04\n",
    "                                                   interest_state_insurance_df.iloc[:,3].values + \n",
    "                                                   interest_state_insurance_df.iloc[:,4].values)- interest_UI_rev\n",
    "interest_state_insurance_df = interest_state_insurance_df.drop(columns=['Worker Comp-Oth Ctrib (Y11)'])\n",
    "interest_state_insurance_df = interest_state_insurance_df.drop(columns=['Oth In Trust-Oth Ctrib (Y51)'])\n",
    "interest_state_insurance_df = interest_state_insurance_df.pivot(index = 'Name' , columns = 'Year4')\n",
    "interest_state_insurance_df.index.values[9] = 'DC STATE GOVT'\n",
    "index_name = interest_state_insurance_df.index.copy()\n",
    "index_name = np.delete(index_name, [1,13,46], None)\n",
    "interest_state_insurance_df = interest_state_insurance_df.reindex(index_name)\n",
    "interest_state_insurance_df.index.values[:] = interest_state_insurance_df.index.astype(str).str[0:2]\n",
    "interest_state_insurance_df = interest_state_insurance_df.reindex(State_abbrev.iloc[3:54,3].values)\n",
    "interest_state_insurance_df.iloc[8,71] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interest_state_insurance_govt_df = Govt_Revenues.copy()\n",
    "interest_state_insurance_govt_df = interest_state_insurance_govt_df.iloc[:,[2,6,92,94,95,96]]\n",
    "interest_state_insurance_govt_df.iloc[:,2].values[:] = (interest_state_insurance_govt_df.iloc[:,2].values +\n",
    "                                                       interest_state_insurance_govt_df.iloc[:,3].values - \n",
    "                                                       interest_state_insurance_govt_df.iloc[:,4].values + \n",
    "                                                       interest_state_insurance_govt_df.iloc[:,5].values)\n",
    "interest_state_insurance_govt_df = interest_state_insurance_govt_df.iloc[:,:3]\n",
    "interest_state_insurance_govt_df1 = interest_state_insurance_govt_df[interest_state_insurance_govt_df[\n",
    "    'Name'].str.contains('STATE-LOCAL TOTAL')==True]\n",
    "interest_state_insurance_govt_df_pivot = interest_state_insurance_govt_df1.pivot(index = 'Name' , columns = 'Year4')\n",
    "index_name = interest_state_insurance_govt_df_pivot.index.copy()\n",
    "index_name = np.delete(index_name, [1,13,46], None)\n",
    "interest_state_insurance_govt_df_pivot = interest_state_insurance_govt_df_pivot.reindex(index_name)\n",
    "interest_state_insurance_govt_df_pivot.index.values[:] = interest_state_insurance_govt_df_pivot.index.astype(str).str[0:2]\n",
    "interest_state_insurance_govt_df_pivot = interest_state_insurance_govt_df_pivot.reindex(State_abbrev.iloc[3:54,3].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct for 2001 and 2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "US_total_interest_state_insurance_govt= interest_state_insurance_govt_df[interest_state_insurance_govt_df['Name'].str.contains('US STATE-LOCAL TOTAL')==True]\n",
    "US_total_interest_state_insurance_govt_2001 = US_total_interest_state_insurance_govt.iloc[7,2]\n",
    "US_total_interest_state_insurance_govt_2003 = US_total_interest_state_insurance_govt.iloc[5,2]\n",
    "interest_state_insurance_govt_df_pivot['Emp Ret-Int Rev (X08)'][2003].values[:] =  (interest_state_insurance_govt_df_pivot['Emp Ret-Int Rev (X08)'][2002].values[:]/\n",
    "                                                                interest_state_insurance_govt_df_pivot['Emp Ret-Int Rev (X08)'][2002].values.sum() *\n",
    "                                                                US_total_interest_state_insurance_govt_2003)\n",
    "interest_state_insurance_govt_df_pivot['Emp Ret-Int Rev (X08)'][2001].values[:] =  (interest_state_insurance_govt_df_pivot['Emp Ret-Int Rev (X08)'][2000].values[:]/\n",
    "                                                                interest_state_insurance_govt_df_pivot['Emp Ret-Int Rev (X08)'][2000].values.sum() * \n",
    "                                                               US_total_interest_state_insurance_govt_2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interest_state_insurance_df = interest_state_insurance_govt_df_pivot.iloc[:,-40:] - interest_state_insurance_df.iloc[:,-40:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2009 \n",
    "interest_state_insurance_09_X08 = np.array(list(map(float, State_Finance_09_pivot['Dollar Amount']['X08'].values)))\n",
    "#State_Finance_09_pivot['Dollar Amount']['Y02'].values[:] = State_Finance_09_pivot['Dollar Amount']['Y02'].fillna(value=0)\n",
    "#interest_state_insurance_09_Y02 = np.array(list(map(float, State_Finance_09_pivot['Dollar Amount']['Y02'].values)))\n",
    "State_Finance_09_pivot['Dollar Amount']['Y12'].values[:] = State_Finance_09_pivot['Dollar Amount']['Y12'].fillna(value=0)\n",
    "interest_state_insurance_09_Y12 = np.array(list(map(float, State_Finance_09_pivot['Dollar Amount']['Y12'].values)))\n",
    "State_Finance_09_pivot['Dollar Amount']['Y52'].values[:] = State_Finance_09_pivot['Dollar Amount']['Y52'].fillna(value=0)\n",
    "interest_state_insurance_09_Y52 = np.array(list(map(float, State_Finance_09_pivot['Dollar Amount']['Y52'].values)))\n",
    "interest_state_insurance_09 = (interest_state_insurance_09_X08  + interest_state_insurance_09_Y12\n",
    "         + interest_state_insurance_09_Y52)\n",
    "interest_state_insurance_09_aug = np.zeros((51))\n",
    "interest_state_insurance_09_aug[:8] = interest_state_insurance_09[1:9]\n",
    "interest_state_insurance_09_aug[9:] = interest_state_insurance_09[9:]\n",
    "interest_state_insurance_df['Tot Ins Trust Inv Rev',2009] = interest_state_insurance_09_aug\n",
    "# 2010 \n",
    "interest_state_insurance_10_X08 = np.array(list(map(float, State_Finance_10_pivot['Dollar Amount']['X08'].values)))\n",
    "#State_Finance_10_pivot['Dollar Amount']['Y02'].values[:] = State_Finance_10_pivot['Dollar Amount']['Y02'].fillna(value=0)\n",
    "#interest_state_insurance_10_Y02 = np.array(list(map(float, State_Finance_10_pivot['Dollar Amount']['Y02'].values)))\n",
    "State_Finance_10_pivot['Dollar Amount']['Y12'].values[:] = State_Finance_10_pivot['Dollar Amount']['Y12'].fillna(value=0)\n",
    "interest_state_insurance_10_Y12 = np.array(list(map(float, State_Finance_10_pivot['Dollar Amount']['Y12'].values)))\n",
    "State_Finance_10_pivot['Dollar Amount']['Y52'].values[:] = State_Finance_10_pivot['Dollar Amount']['Y52'].fillna(value=0)\n",
    "interest_state_insurance_10_Y52 = np.array(list(map(float, State_Finance_10_pivot['Dollar Amount']['Y52'].values)))\n",
    "interest_state_insurance_10 = (interest_state_insurance_10_X08  + interest_state_insurance_10_Y12\n",
    "         + interest_state_insurance_10_Y52)\n",
    "interest_state_insurance_10_aug = np.zeros((51))\n",
    "interest_state_insurance_10_aug[:8] = interest_state_insurance_10[1:9]\n",
    "interest_state_insurance_10_aug[9:] = interest_state_insurance_10[9:]\n",
    "interest_state_insurance_df['Tot Ins Trust Inv Rev',2010] = interest_state_insurance_10_aug\n",
    "# 2011\n",
    "interest_state_insurance_11_X08 = np.array(list(map(float, State_Finance_11_pivot['Dollar Amount']['X08'].values)))\n",
    "#State_Finance_11_pivot['Dollar Amount']['Y02'].values[:] = State_Finance_11_pivot['Dollar Amount']['Y02'].fillna(value=0)\n",
    "#interest_state_insurance_11_Y02 = np.array(list(map(float, State_Finance_11_pivot['Dollar Amount']['Y02'].values)))\n",
    "State_Finance_11_pivot['Dollar Amount']['Y12'].values[:] = State_Finance_11_pivot['Dollar Amount']['Y12'].fillna(value=0)\n",
    "interest_state_insurance_11_Y12 = np.array(list(map(float, State_Finance_11_pivot['Dollar Amount']['Y12'].values)))\n",
    "State_Finance_11_pivot['Dollar Amount']['Y52'].values[:] = State_Finance_11_pivot['Dollar Amount']['Y52'].fillna(value=0)\n",
    "interest_state_insurance_11_Y52 = np.array(list(map(float, State_Finance_11_pivot['Dollar Amount']['Y52'].values)))\n",
    "interest_state_insurance_11 = (interest_state_insurance_11_X08  + interest_state_insurance_11_Y12\n",
    "         + interest_state_insurance_11_Y52)\n",
    "interest_state_insurance_11_aug = np.zeros((51))\n",
    "interest_state_insurance_11_aug[:8] = interest_state_insurance_11[1:9]\n",
    "interest_state_insurance_11_aug[9:] = interest_state_insurance_11[9:]\n",
    "interest_state_insurance_df['Tot Ins Trust Inv Rev',2011] = interest_state_insurance_11_aug\n",
    "# 2012\n",
    "interest_state_insurance_12_X08 = np.array(list(map(float, State_Finance_12_pivot['Dollar Amount']['X08'].values)))\n",
    "#State_Finance_12_pivot['Dollar Amount']['Y02'].values[:] = State_Finance_12_pivot['Dollar Amount']['Y02'].fillna(value=0)\n",
    "#interest_state_insurance_12_Y02 = np.array(list(map(float, State_Finance_12_pivot['Dollar Amount']['Y02'].values)))\n",
    "State_Finance_12_pivot['Dollar Amount']['Y12'].values[:] = State_Finance_12_pivot['Dollar Amount']['Y12'].fillna(value=0)\n",
    "interest_state_insurance_12_Y12 = np.array(list(map(float, State_Finance_12_pivot['Dollar Amount']['Y12'].values)))\n",
    "State_Finance_12_pivot['Dollar Amount']['Y52'].values[:] = State_Finance_12_pivot['Dollar Amount']['Y52'].fillna(value=0)\n",
    "interest_state_insurance_12_Y52 = np.array(list(map(float, State_Finance_12_pivot['Dollar Amount']['Y52'].values)))\n",
    "interest_state_insurance_12 = (interest_state_insurance_12_X08  + interest_state_insurance_12_Y12\n",
    "         + interest_state_insurance_12_Y52)\n",
    "interest_state_insurance_12_aug = np.zeros((51))\n",
    "interest_state_insurance_12_aug[:8] = interest_state_insurance_12[1:9]\n",
    "interest_state_insurance_12_aug[9:] = interest_state_insurance_12[9:]\n",
    "interest_state_insurance_df['Tot Ins Trust Inv Rev',2012] = interest_state_insurance_12_aug\n",
    "# 2013\n",
    "interest_state_insurance_13_X08 = np.array(list(map(float, State_Finance_13_pivot['Dollar Amount']['X08'].values)))\n",
    "#State_Finance_13_pivot['Dollar Amount']['Y02'].values[:] = State_Finance_13_pivot['Dollar Amount']['Y02'].fillna(value=0)\n",
    "#interest_state_insurance_13_Y02 = np.array(list(map(float, State_Finance_13_pivot['Dollar Amount']['Y02'].values)))\n",
    "State_Finance_13_pivot['Dollar Amount']['Y12'].values[:] = State_Finance_13_pivot['Dollar Amount']['Y12'].fillna(value=0)\n",
    "interest_state_insurance_13_Y12 = np.array(list(map(float, State_Finance_13_pivot['Dollar Amount']['Y12'].values)))\n",
    "State_Finance_13_pivot['Dollar Amount']['Y52'].values[:] = State_Finance_13_pivot['Dollar Amount']['Y52'].fillna(value=0)\n",
    "interest_state_insurance_13_Y52 = np.array(list(map(float, State_Finance_13_pivot['Dollar Amount']['Y52'].values)))\n",
    "interest_state_insurance_13 = (interest_state_insurance_13_X08  + interest_state_insurance_13_Y12\n",
    "         + interest_state_insurance_13_Y52)\n",
    "interest_state_insurance_13_aug = np.zeros((51))\n",
    "interest_state_insurance_13_aug[:8] = interest_state_insurance_13[1:9]\n",
    "interest_state_insurance_13_aug[9:] = interest_state_insurance_13[9:]\n",
    "interest_state_insurance_df['Tot Ins Trust Inv Rev',2013] = interest_state_insurance_13_aug\n",
    "# 2014\n",
    "interest_state_insurance_14_X08 = np.array(list(map(float, State_Finance_14_pivot['Dollar Amount']['X08'].values)))\n",
    "#State_Finance_14_pivot['Dollar Amount']['Y02'].values[:] = State_Finance_14_pivot['Dollar Amount']['Y02'].replace('', 0, regex=True)\n",
    "#interest_state_insurance_14_Y02 = np.array(list(map(float, State_Finance_14_pivot['Dollar Amount']['Y02'].values)))\n",
    "State_Finance_14_pivot['Dollar Amount']['Y12'].values[:] = State_Finance_14_pivot['Dollar Amount']['Y12'].fillna(value=0).replace('', 0, regex=True)\n",
    "interest_state_insurance_14_Y12 = np.array(list(map(float, State_Finance_14_pivot['Dollar Amount']['Y12'].values)))\n",
    "State_Finance_14_pivot['Dollar Amount']['Y52'].values[:] = State_Finance_14_pivot['Dollar Amount']['Y52'].fillna(value=0).replace('', 0, regex=True)\n",
    "interest_state_insurance_14_Y52 = np.array(list(map(float, State_Finance_14_pivot['Dollar Amount']['Y52'].values)))\n",
    "interest_state_insurance_14 = (interest_state_insurance_14_X08  + interest_state_insurance_14_Y12\n",
    "         + interest_state_insurance_14_Y52)\n",
    "interest_state_insurance_14_aug = np.zeros((51))\n",
    "interest_state_insurance_14_aug[:8] = interest_state_insurance_14[1:9]\n",
    "interest_state_insurance_14_aug[9:] = interest_state_insurance_14[9:]\n",
    "interest_state_insurance_df['Tot Ins Trust Inv Rev',2014] = interest_state_insurance_14_aug\n",
    "# 2015\n",
    "interest_state_insurance_15_X08 = np.array(list(map(float, State_Finance_15_pivot['Dollar Amount']['X08'].values)))\n",
    "#State_Finance_15_pivot['Dollar Amount']['Y02'].values[:] = State_Finance_15_pivot['Dollar Amount']['Y02'].replace('', 0, regex=True)\n",
    "#interest_state_insurance_15_Y02 = np.array(list(map(float, State_Finance_15_pivot['Dollar Amount']['Y02'].values)))\n",
    "State_Finance_15_pivot['Dollar Amount']['Y12'].values[:] = State_Finance_15_pivot['Dollar Amount']['Y12'].fillna(value=0).replace('', 0, regex=True)\n",
    "interest_state_insurance_15_Y12 = np.array(list(map(float, State_Finance_15_pivot['Dollar Amount']['Y12'].values)))\n",
    "State_Finance_15_pivot['Dollar Amount']['Y52'].values[:] = State_Finance_15_pivot['Dollar Amount']['Y52'].fillna(value=0).replace('', 0, regex=True)\n",
    "interest_state_insurance_15_Y52 = np.array(list(map(float, State_Finance_15_pivot['Dollar Amount']['Y52'].values)))\n",
    "interest_state_insurance_15 = (interest_state_insurance_15_X08  + interest_state_insurance_15_Y12\n",
    "         + interest_state_insurance_15_Y52)\n",
    "interest_state_insurance_15_aug = np.zeros((51))\n",
    "interest_state_insurance_15_aug[:8] = interest_state_insurance_15[1:9]\n",
    "interest_state_insurance_15_aug[10:] = interest_state_insurance_15[9:]\n",
    "interest_state_insurance_df['Tot Ins Trust Inv Rev',2015] = interest_state_insurance_15_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miscellaneous interest earnings =  U20 + U21 + U40 + U41. Again, no local, only state level data - yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "misc_interest = (State_Revenues['Interest Revenue (U20)'].values + State_Revenues['Dividends on Investments (U21)'].values + \n",
    "                State_Revenues['Rents (U40)'].values + State_Revenues['Royalties (U41)'].values)\n",
    "misc_interest_df = State_Revenues.copy()\n",
    "misc_interest_df = misc_interest_df.iloc[:,[2,7,124]]\n",
    "misc_interest_df.iloc[:,2].values[:] = misc_interest\n",
    "misc_interest_df.rename(columns={'Interest Revenue (U20)': 'Misc. interest earnings'}, inplace=True)\n",
    "misc_interest_df = misc_interest_df.pivot(index = 'Name' , columns = 'Year4')\n",
    "misc_interest_df.index.values[9] = 'DC STATE GOVT'\n",
    "index_name = misc_interest_df.index.copy()\n",
    "index_name = np.delete(index_name, [1,13,46], None)\n",
    "misc_interest_df = misc_interest_df.reindex(index_name)\n",
    "misc_interest_df.index.values[:] =misc_interest_df.index.astype(str).str[0:2]\n",
    "misc_interest_df = misc_interest_df.reindex(State_abbrev.iloc[3:54,3].values)\n",
    "misc_interest_df.iloc[8,71] = 0\n",
    "# 2009 \n",
    "misc_interest_09_U20 = np.array(list(map(float, State_Finance_09_pivot['Dollar Amount']['U20'].values)))\n",
    "State_Finance_09_pivot['Dollar Amount']['U21'].values[:] = State_Finance_09_pivot['Dollar Amount']['U21'].fillna(value=0)\n",
    "misc_interest_09_U21 = np.array(list(map(float, State_Finance_09_pivot['Dollar Amount']['U21'].values)))\n",
    "State_Finance_09_pivot['Dollar Amount']['U40'].values[:] = State_Finance_09_pivot['Dollar Amount']['U40'].fillna(value=0)\n",
    "misc_interest_09_U40 = np.array(list(map(float, State_Finance_09_pivot['Dollar Amount']['U40'].values)))\n",
    "State_Finance_09_pivot['Dollar Amount']['U41'].values[:] = State_Finance_09_pivot['Dollar Amount']['U41'].fillna(value=0)\n",
    "misc_interest_09_U41 = np.array(list(map(float, State_Finance_09_pivot['Dollar Amount']['U41'].values)))\n",
    "misc_interest_09 = (misc_interest_09_U20 + misc_interest_09_U21 + misc_interest_09_U40\n",
    "         + misc_interest_09_U41)\n",
    "misc_interest_09_aug = np.zeros((51))\n",
    "misc_interest_09_aug[:8] = misc_interest_09[1:9]\n",
    "misc_interest_09_aug[9:] = misc_interest_09[9:]\n",
    "misc_interest_df['Misc. interest earnings',2009] = misc_interest_09_aug\n",
    "# 2010 \n",
    "misc_interest_10_U20 = np.array(list(map(float, State_Finance_10_pivot['Dollar Amount']['U20'].values)))\n",
    "State_Finance_10_pivot['Dollar Amount']['U21'].values[:] = State_Finance_10_pivot['Dollar Amount']['U21'].fillna(value=0)\n",
    "misc_interest_10_U21 = np.array(list(map(float, State_Finance_10_pivot['Dollar Amount']['U21'].values)))\n",
    "State_Finance_10_pivot['Dollar Amount']['U40'].values[:] = State_Finance_10_pivot['Dollar Amount']['U40'].fillna(value=0)\n",
    "misc_interest_10_U40 = np.array(list(map(float, State_Finance_10_pivot['Dollar Amount']['U40'].values)))\n",
    "State_Finance_10_pivot['Dollar Amount']['U41'].values[:] = State_Finance_10_pivot['Dollar Amount']['U41'].fillna(value=0)\n",
    "misc_interest_10_U41 = np.array(list(map(float, State_Finance_10_pivot['Dollar Amount']['U41'].values)))\n",
    "misc_interest_10 = (misc_interest_10_U20 + misc_interest_10_U21 + misc_interest_10_U40\n",
    "         + misc_interest_10_U41)\n",
    "misc_interest_10_aug = np.zeros((51))\n",
    "misc_interest_10_aug[:8] = misc_interest_10[1:9]\n",
    "misc_interest_10_aug[9:] = misc_interest_10[9:]\n",
    "misc_interest_df['Misc. interest earnings',2010] = misc_interest_10_aug\n",
    "# 2011\n",
    "misc_interest_11_U20 = np.array(list(map(float, State_Finance_11_pivot['Dollar Amount']['U20'].values)))\n",
    "State_Finance_11_pivot['Dollar Amount']['U21'].values[:] = State_Finance_11_pivot['Dollar Amount']['U21'].fillna(value=0)\n",
    "misc_interest_11_U21 = np.array(list(map(float, State_Finance_11_pivot['Dollar Amount']['U21'].values)))\n",
    "State_Finance_11_pivot['Dollar Amount']['U40'].values[:] = State_Finance_11_pivot['Dollar Amount']['U40'].fillna(value=0)\n",
    "misc_interest_11_U40 = np.array(list(map(float, State_Finance_11_pivot['Dollar Amount']['U40'].values)))\n",
    "State_Finance_11_pivot['Dollar Amount']['U41'].values[:] = State_Finance_11_pivot['Dollar Amount']['U41'].fillna(value=0)\n",
    "misc_interest_11_U41 = np.array(list(map(float, State_Finance_11_pivot['Dollar Amount']['U41'].values)))\n",
    "misc_interest_11 = (misc_interest_11_U20 + misc_interest_11_U21 + misc_interest_11_U40\n",
    "         + misc_interest_11_U41)\n",
    "misc_interest_11_aug = np.zeros((51))\n",
    "misc_interest_11_aug[:8] = misc_interest_11[1:9]\n",
    "misc_interest_11_aug[9:] = misc_interest_11[9:]\n",
    "misc_interest_df['Misc. interest earnings',2011] = misc_interest_11_aug\n",
    "# 2012\n",
    "misc_interest_12_U20 = np.array(list(map(float, State_Finance_12_pivot['Dollar Amount']['U20'].values)))\n",
    "State_Finance_12_pivot['Dollar Amount']['U21'].values[:] = State_Finance_12_pivot['Dollar Amount']['U21'].fillna(value=0)\n",
    "misc_interest_12_U21 = np.array(list(map(float, State_Finance_12_pivot['Dollar Amount']['U21'].values)))\n",
    "State_Finance_12_pivot['Dollar Amount']['U40'].values[:] = State_Finance_12_pivot['Dollar Amount']['U40'].fillna(value=0)\n",
    "misc_interest_12_U40 = np.array(list(map(float, State_Finance_12_pivot['Dollar Amount']['U40'].values)))\n",
    "State_Finance_12_pivot['Dollar Amount']['U41'].values[:] = State_Finance_12_pivot['Dollar Amount']['U41'].fillna(value=0)\n",
    "misc_interest_12_U41 = np.array(list(map(float, State_Finance_12_pivot['Dollar Amount']['U41'].values)))\n",
    "misc_interest_12 = (misc_interest_12_U20 + misc_interest_12_U21 + misc_interest_12_U40\n",
    "         + misc_interest_12_U41)\n",
    "misc_interest_12_aug = np.zeros((51))\n",
    "misc_interest_12_aug[:8] = misc_interest_12[1:9]\n",
    "misc_interest_12_aug[9:] = misc_interest_12[9:]\n",
    "misc_interest_df['Misc. interest earnings',2012] = misc_interest_12_aug\n",
    "# 2013\n",
    "misc_interest_13_U20 = np.array(list(map(float, State_Finance_13_pivot['Dollar Amount']['U20'].values)))\n",
    "State_Finance_13_pivot['Dollar Amount']['U21'].values[:] = State_Finance_13_pivot['Dollar Amount']['U21'].fillna(value=0)\n",
    "misc_interest_13_U21 = np.array(list(map(float, State_Finance_13_pivot['Dollar Amount']['U21'].values)))\n",
    "State_Finance_13_pivot['Dollar Amount']['U40'].values[:] = State_Finance_13_pivot['Dollar Amount']['U40'].fillna(value=0)\n",
    "misc_interest_13_U40 = np.array(list(map(float, State_Finance_13_pivot['Dollar Amount']['U40'].values)))\n",
    "State_Finance_13_pivot['Dollar Amount']['U41'].values[:] = State_Finance_13_pivot['Dollar Amount']['U41'].fillna(value=0)\n",
    "misc_interest_13_U41 = np.array(list(map(float, State_Finance_13_pivot['Dollar Amount']['U41'].values)))\n",
    "misc_interest_13 = (misc_interest_13_U20 + misc_interest_13_U21 + misc_interest_13_U40\n",
    "         + misc_interest_13_U41)\n",
    "misc_interest_13_aug = np.zeros((51))\n",
    "misc_interest_13_aug[:8] = misc_interest_13[1:9]\n",
    "misc_interest_13_aug[9:] = misc_interest_13[9:]\n",
    "misc_interest_df['Misc. interest earnings',2013] = misc_interest_13_aug\n",
    "# 2014\n",
    "misc_interest_14_U20 = np.array(list(map(float, State_Finance_14_pivot['Dollar Amount']['U20'].values)))\n",
    "State_Finance_14_pivot['Dollar Amount']['U21'].values[:] = State_Finance_14_pivot['Dollar Amount']['U21'].fillna(value=0).replace('', 0, regex=True)\n",
    "misc_interest_14_U21 = np.array(list(map(float, State_Finance_14_pivot['Dollar Amount']['U21'].values)))\n",
    "State_Finance_14_pivot['Dollar Amount']['U40'].values[:] = State_Finance_14_pivot['Dollar Amount']['U40'].fillna(value=0).replace('', 0, regex=True)\n",
    "misc_interest_14_U40 = np.array(list(map(float, State_Finance_14_pivot['Dollar Amount']['U40'].values)))\n",
    "State_Finance_14_pivot['Dollar Amount']['U41'].values[:] = State_Finance_14_pivot['Dollar Amount']['U41'].fillna(value=0).replace('', 0, regex=True)\n",
    "misc_interest_14_U41 = np.array(list(map(float, State_Finance_14_pivot['Dollar Amount']['U41'].values)))\n",
    "misc_interest_14 = (misc_interest_14_U20 + misc_interest_14_U21 + misc_interest_14_U40\n",
    "         + misc_interest_14_U41)\n",
    "misc_interest_14_aug = np.zeros((51))\n",
    "misc_interest_14_aug[:8] = misc_interest_14[1:9]\n",
    "misc_interest_14_aug[9:] = misc_interest_14[9:]\n",
    "misc_interest_df['Misc. interest earnings',2014] = misc_interest_14_aug\n",
    "# 2015\n",
    "misc_interest_15_U20 = np.array(list(map(float, State_Finance_15_pivot['Dollar Amount']['U20'].values)))\n",
    "State_Finance_15_pivot['Dollar Amount']['U21'].values[:] = State_Finance_15_pivot['Dollar Amount']['U21'].fillna(value=0).replace('', 0, regex=True)\n",
    "misc_interest_15_U21 = np.array(list(map(float, State_Finance_15_pivot['Dollar Amount']['U21'].values)))\n",
    "State_Finance_15_pivot['Dollar Amount']['U40'].values[:] = State_Finance_15_pivot['Dollar Amount']['U40'].fillna(value=0).replace('', 0, regex=True)\n",
    "misc_interest_15_U40 = np.array(list(map(float, State_Finance_15_pivot['Dollar Amount']['U40'].values)))\n",
    "State_Finance_15_pivot['Dollar Amount']['U41'].values[:] = State_Finance_15_pivot['Dollar Amount']['U41'].fillna(value=0).replace('', 0, regex=True)\n",
    "misc_interest_15_U41 = np.array(list(map(float, State_Finance_15_pivot['Dollar Amount']['U41'].values)))\n",
    "misc_interest_15 = (misc_interest_15_U20 + misc_interest_15_U21 + misc_interest_15_U40\n",
    "         + misc_interest_15_U41)\n",
    "misc_interest_15_aug = np.zeros((51))\n",
    "misc_interest_15_aug[:8] = misc_interest_15[1:9]\n",
    "misc_interest_15_aug[10:] = misc_interest_15[9:]\n",
    "misc_interest_df['Misc. interest earnings',2015] = misc_interest_15_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interest_state = misc_interest_df.iloc[:,-47:] + interest_state_insurance_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_vector2 = [s for s in names] + [s + '*' for s in names]\n",
    "SA35_table3 = SA35_table1.loc[names_vector2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "payments_indiv = SA35_table3.iloc[37::40,:].applymap(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "federal_transfers = SA35_table3.iloc[32::40,:].applymap(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_local_transfers = SA35_table3.iloc[31::40,:].applymap(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transfers = (payments_indiv + federal_transfers.values + state_local_transfers.values) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_federal_state_income = (Personal_nominal_income/1000 + federal_non_personal_taxes.values + \n",
    "                            state_local_non_personal_taxes.values+interest_state.values/ 1000 -transfers.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federal grants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fed_grants = Govt_Revenues.loc[:,'Total Fed IG Revenue']\n",
    "fed_grants_df = Govt_Revenues.copy()\n",
    "fed_grants_df = fed_grants_df.iloc[:,[2,6,47]]\n",
    "fed_grants_df1 = fed_grants_df[fed_grants_df['Name'].str.contains('STATE-LOCAL TOTAL')==True]\n",
    "fed_grants_df_pivot = fed_grants_df1.pivot(index = 'Name' , columns = 'Year4')\n",
    "index_name = fed_grants_df_pivot.index.copy()\n",
    "index_name = np.delete(index_name, [1,13,46], None)\n",
    "fed_grants_df_pivot = fed_grants_df_pivot.reindex(index_name)\n",
    "fed_grants_df_pivot.index.values[:] = fed_grants_df_pivot.index.astype(str).str[0:2]\n",
    "fed_grants_df_pivot = fed_grants_df_pivot.reindex(State_abbrev.iloc[3:54,3].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, correct for the missing data in 2001 and 2003 the same way as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "US_total_fed_grants= fed_grants_df[fed_grants_df['Name'].str.contains('US STATE GOVTS')==True]\n",
    "US_total_fed_grants_2001 = US_total_fed_grants.iloc[7,2]\n",
    "US_total_fed_grants_2003 = US_total_fed_grants.iloc[5,2]\n",
    "fed_grants_df_pivot['Total Fed IG Revenue'][2003].values[:] =  (fed_grants_df_pivot['Total Fed IG Revenue'][2002].values[:]/\n",
    "                                                                fed_grants_df_pivot['Total Fed IG Revenue'][2002].values.sum() * US_total_fed_grants_2003)\n",
    "fed_grants_df_pivot['Total Fed IG Revenue'][2001].values[:] =  (fed_grants_df_pivot['Total Fed IG Revenue'][2000].values[:]/\n",
    "                                                                fed_grants_df_pivot['Total Fed IG Revenue'][2000].values.sum() * US_total_fed_grants_2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data from 2009-2015\n",
    "\n",
    "fed_grants_df_pivot['Total Fed IG Revenue',2009] =pd.concat([State_Local_Finance_09.iloc[17,8:132:5],\n",
    "                                                               State_Local_Finance_09.iloc[17,134::5]] , axis=0).values[:]\n",
    "fed_grants_df_pivot['Total Fed IG Revenue',2010] =pd.concat([State_Local_Finance_10.iloc[17,8:132:5],\n",
    "                                                               State_Local_Finance_10.iloc[17,134::5]] , axis=0).values[:]\n",
    "fed_grants_df_pivot['Total Fed IG Revenue',2011] =pd.concat([State_Local_Finance_11.iloc[17,8:132:5],\n",
    "                                                               State_Local_Finance_11.iloc[17,134::5]] , axis=0).values[:]\n",
    "fed_grants_df_pivot['Total Fed IG Revenue',2012] =pd.concat([State_Local_Finance_12.iloc[17,6:81:3],\n",
    "                                                             State_Local_Finance_12.iloc[17,83::3]] , axis=0).values[:]\n",
    "fed_grants_df_pivot['Total Fed IG Revenue',2013] =pd.concat([State_Local_Finance_13.iloc[19,9:132:5],\n",
    "                                                             State_Local_Finance_13.iloc[19,136::5]] , axis=0).values[:]\n",
    "fed_grants_df_pivot['Total Fed IG Revenue',2014] =pd.concat([State_Local_Finance_14.iloc[19,9:132:5],\n",
    "                                                             State_Local_Finance_14.iloc[19,136::5]] , axis=0).values[:]\n",
    "fed_grants_df_pivot['Total Fed IG Revenue',2015] =pd.concat([State_Local_Finance_15.iloc[20,9:132:5],\n",
    "                                                             State_Local_Finance_15.iloc[20,136::5]] , axis=0).values[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fed_grants_df_pivot = fed_grants_df_pivot.iloc[:,-47:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federal transfers & personal taxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Social_security_benefits = SA35_table3.iloc[19::40,:].applymap(float)\n",
    "railroad = SA35_table3.iloc[6::40,:].applymap(float)\n",
    "worker_comp = SA35_table3.iloc[10::40,:].applymap(float)\n",
    "medical_benefits = SA35_table3.iloc[28::40,:].applymap(float)\n",
    "ssi_benefits = SA35_table3.iloc[22::40,:].applymap(float)\n",
    "snap_benefits = SA35_table3.iloc[21::40,:]\n",
    "snap_benefits = snap_benefits.replace(to_replace='(L)', value=0) #equivalent to snap_benefits.iloc[21,0] = 0 \n",
    "snap_benefits = snap_benefits.applymap(float)\n",
    "other_income_benefits  =  SA35_table3.iloc[17::40,:].applymap(float) # this one is completely different and larger than in ASY\n",
    "UI_benefits = SA35_table3.iloc[34::40,:].applymap(float)\n",
    "veteran_benefits = SA35_table3.iloc[35::40,:].applymap(float)\n",
    "education_benefits = SA35_table3.iloc[26::40,:].applymap(float) # almost double the amount than in ASY\n",
    "medicaid = SA35_table3.iloc[2::40,:].replace(to_replace='(NA)', value=0).applymap(float) # similar to ASY, but taken from BEA instead\n",
    "fed_transfers_indiv = (Social_security_benefits.values + railroad.values + worker_comp.values + medical_benefits.values + \n",
    "                       ssi_benefits.values + snap_benefits.values + other_income_benefits.values + \n",
    "                       UI_benefits.values + veteran_benefits.values + education_benefits.values + \n",
    "                      federal_transfers - medicaid.values) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "federal_personal_tax = SA50_table3[7::18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disposable state income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disposable_state_income = (non_federal_state_income + fed_grants_df_pivot.values /1000 + fed_transfers_indiv.values - \n",
    "                          federal_non_personal_taxes.values - federal_personal_tax.values/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Government expenditure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloaded from https://www2.census.gov/pub/outgoing/govs/special60/Govt_Finances.zip\n",
    "Govt_expenditures = pd.read_excel('Govt_Expenditure.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expenditure_df = Govt_expenditures.copy()\n",
    "expenditure_df = expenditure_df.iloc[:,[2,6,21]]\n",
    "\n",
    "#interest_state_insurance_df.iloc[8,71] = 0\n",
    "expenditure_df1 = expenditure_df[expenditure_df['Name'].str.contains('STATE-LOCAL TOTAL')==True]\n",
    "expenditure_df_pivot = expenditure_df1.pivot(index = 'Name' , columns = 'Year4')\n",
    "index_name = expenditure_df_pivot.index.copy()\n",
    "index_name = np.delete(index_name, [1,13,46], None)\n",
    "expenditure_df_pivot = expenditure_df_pivot.reindex(index_name)\n",
    "expenditure_df_pivot.index.values[:] = expenditure_df_pivot.index.astype(str).str[0:2]\n",
    "expenditure_df_pivot = expenditure_df_pivot.reindex(State_abbrev.iloc[3:54,3].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "US_total_expenditure= expenditure_df[expenditure_df['Name'].str.contains('US STATE-LOCAL TOTAL')==True]\n",
    "US_total_expenditure_2001 = US_total_expenditure.iloc[7,2]\n",
    "US_total_expenditure_2003 = US_total_expenditure.iloc[5,2]\n",
    "expenditure_df_pivot['Direct General Expend'][2003].values[:] =  (expenditure_df_pivot['Direct General Expend'][2002].values[:]/\n",
    "                                                                expenditure_df_pivot['Direct General Expend'][2002].values.sum() *\n",
    "                                                                US_total_expenditure_2003)\n",
    "expenditure_df_pivot['Direct General Expend'][2001].values[:] =  (expenditure_df_pivot['Direct General Expend'][2000].values[:]/\n",
    "                                                                expenditure_df_pivot['Direct General Expend'][2000].values.sum() * \n",
    "                                                               US_total_expenditure_2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data from 2009-2015\n",
    "\n",
    "expenditure_df_pivot['Direct General Expend',2009] =pd.concat([State_Local_Finance_09.iloc[89,6:130:5],\n",
    "                                                               State_Local_Finance_09.iloc[89,132::5]] , axis=0).values[:]\n",
    "expenditure_df_pivot['Direct General Expend',2010] =pd.concat([State_Local_Finance_10.iloc[89,6:130:5],\n",
    "                                                               State_Local_Finance_10.iloc[89,132::5]] , axis=0).values[:]\n",
    "expenditure_df_pivot['Direct General Expend',2011] =pd.concat([State_Local_Finance_11.iloc[89,6:130:5],\n",
    "                                                               State_Local_Finance_11.iloc[89,132::5]] , axis=0).values[:]\n",
    "expenditure_df_pivot['Direct General Expend',2012] =pd.concat([State_Local_Finance_12.iloc[89,5:80:3],\n",
    "                                                             State_Local_Finance_12.iloc[89,82::3]] , axis=0).values[:]\n",
    "expenditure_df_pivot['Direct General Expend',2013] =pd.concat([State_Local_Finance_13.iloc[91,7:130:5],\n",
    "                                                             State_Local_Finance_13.iloc[91,134::5]] , axis=0).values[:]\n",
    "expenditure_df_pivot['Direct General Expend',2014] =pd.concat([State_Local_Finance_14.iloc[91,7:130:5],\n",
    "                                                             State_Local_Finance_14.iloc[91,134::5]] , axis=0).values[:]\n",
    "expenditure_df_pivot['Direct General Expend',2015] =pd.concat([State_Local_Finance_15.iloc[92,7:130:5],\n",
    "                                                             State_Local_Finance_15.iloc[92,134::5]] , axis=0).values[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expenditure_df_pivot = expenditure_df_pivot.iloc[:,-47:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_local_transfers = transfers - fed_transfers_indiv.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_gov_cons = expenditure_df_pivot / 1000 - state_local_transfers.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State shares of personal consumption - here I differ significantly from the original paper, as there have been significant improvements in the state/federal level consumption data. Namely prior to 1997, I am going to use their weights to construct personal consumption, however, after 1997, I will use the values published by the BEA. Prior to 1997 I use the updated series for US total personal consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "US_consumption_exp = Quandl.get(\"FRED/PCEC\", authtoken=\"5QphWABG_zpJsB5dy4yW\", collapse=\"annual\", end_date=\"2017-06-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://apps.bea.gov/regional/zip/PCEbyState.zip')\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "f = z.open('PCE_ALL_AREAS.csv')\n",
    "PCE_table = pd.read_csv(f,encoding = \"ISO-8859-1\", low_memory=False)\n",
    "cons_weights = pd.read_excel('nch16.xls',header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep = np.arange(8,28)\n",
    "keep = keep.tolist()\n",
    "keep.append(1)\n",
    "keep.append(7)\n",
    "data = PCE_table.iloc[:,keep]\n",
    "data = data[0:1248] #some useless columns\n",
    "data = data.drop_duplicates(['GeoName' ,'Description'])\n",
    "p = data.pivot(index = 'GeoName' , columns = 'Description')\n",
    "PCE_table1 = p.stack()\n",
    "PCE_table2 = PCE_table1.iloc[23::24,:].reset_index(level=1, drop=True)\n",
    "PCE_table2.index.names = ['state']\n",
    "PCE_table2 = PCE_table2.drop(['United States'])\n",
    "p = cons_weights.pivot(index = 'state' , columns = 'Region')\n",
    "cons_weights1 = p.stack()\n",
    "cons_weights1 = cons_weights1.reset_index(level=1, drop=True)\n",
    "cons_weights1  = cons_weights1.iloc[:,:-2]\n",
    "cons_weights1.index = PCE_table2.index\n",
    "PCE_table3 = pd.concat([cons_weights1, PCE_table2], axis=1)\n",
    "cons_weights2 = PCE_table3 / PCE_table3.values.sum(0)\n",
    "PCE_table4 = cons_weights2 * np.tile(US_consumption_exp.values[16:-1].T,(51,1)) * 1000\n",
    "PCE_table5 = PCE_table4.iloc[:,-48:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_consumption = PCE_table5 + state_gov_cons.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load IRS migration rates = use the database provided by Janine Billadello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mig_table_inflow = state_consumption.copy()\n",
    "mig_table_inflow.iloc[:,:] = 0\n",
    "mig_table_outflow = mig_table_inflow.copy()\n",
    "mig_table_nonmig = mig_table_inflow.copy()\n",
    "state_index_ver = state_consumption.index.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://faculty.baruch.cuny.edu/geoportal/data/irs_migration/irsmig_state_database.zip')\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "f = z.open(z.filelist[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "State_name_index_abbr = State_abbrev.iloc[3:54,3].values\n",
    "#State_name_index_abbr = [s + ' Total Migration-US and Foreign' for s in State_name_index_abbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"irs_migration_state.sqlite\")\n",
    "\n",
    "cursor = con.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "results_name = cursor.fetchall()\n",
    "results_name1 = results_name[18:-1]\n",
    "for i in range(len(results_name1)):\n",
    "    results_name1[i] = str(results_name1[i])[2:-3]\n",
    "results_name1.sort()\n",
    "for i in range(int(len(results_name1)/2)):\n",
    "    #print(results_name1[2*i])\n",
    "    df = pd.read_sql_query(\"SELECT * from \" + results_name1[2*i], con)\n",
    "    if i>= int(len(results_name1)/4):\n",
    "        df = df.set_index('st_dest_name')\n",
    "        nonmig_df = df[df.index.str.contains(\"Migrant\") == True]\n",
    "        nonmig_df = nonmig_df[nonmig_df['st_dest_abbrv'].str.contains(\"FR\")== False]\n",
    "        #mig_table_nonmig.iloc[:10,19 + int(i-len(results_name1)/4)] = nonmig_df.values[:10,5]\n",
    "        #mig_table_nonmig.iloc[10:,19 + int(i-len(results_name1)/4)] = nonmig_df.values[11:,5]\n",
    "        if nonmig_df.shape[0] == 51:\n",
    "            mig_table_nonmig.iloc[:,int(19 + int(i-len(results_name1)/4))] = nonmig_df.values[:,5]\n",
    "    else:\n",
    "        df = df.set_index('st_orig_name')\n",
    "        nonmig_df = df[df.index.str.contains(\"Migrant\") == True]\n",
    "        nonmig_df = nonmig_df[nonmig_df['st_dest_abbrv'].str.contains(\"FR\")== False]\n",
    "        #mig_table_nonmig.iloc[:10,19 + int(i-len(results_name1)/4)] = nonmig_df.values[:10,5]\n",
    "        #mig_table_nonmig.iloc[10:,19 + int(i-len(results_name1)/4)] = nonmig_df.values[11:,5]\n",
    "        if nonmig_df.shape[0] == 51:\n",
    "            mig_table_nonmig.iloc[:,int(19 + i)] = nonmig_df.values[:,5]\n",
    "for i in range(int(len(results_name1)/2)):\n",
    "    #print(results_name1[2*i+1])\n",
    "    df = pd.read_sql_query(\"SELECT * from \" + results_name1[2*i + 1], con)\n",
    "    if i>= int(len(results_name1)/4):\n",
    "        df = df.set_index('st_orig_abbrv')\n",
    "        total_mig = df[df.index.str.contains(\"Migration\") == True ].iloc[0::3,5]\n",
    "        \n",
    "        if total_mig.shape[0] ==0:\n",
    "            total_mig = df[df['st_dest_abbrv'].str.contains(\"FR\")== False].iloc[:,5]\n",
    "        if total_mig.shape[0] >51:\n",
    "            total_mig = df[df['st_dest_abbrv'].str.contains(\"FR\")== False].iloc[0::3,5]\n",
    "        if total_mig.shape[0] == 51:\n",
    "            total_mig = total_mig.reindex(State_name_index_abbr)\n",
    "            mig_table_outflow.iloc[:,int(19 + int(i-len(results_name1)/4))] = total_mig.values\n",
    "        else:\n",
    "            1#print('outflow') \n",
    "    else:\n",
    "        df = df.set_index('st_orig_abbrv')\n",
    "        total_mig = df[df.index.str.contains(\"Migration\") == True].iloc[0::3,5]\n",
    "        if total_mig.shape[0] ==0:\n",
    "            total_mig = df[df['st_dest_abbrv'].str.contains(\"FR\")== False].iloc[:,5]\n",
    "        if total_mig.shape[0] >51:\n",
    "            total_mig = df[df['st_dest_abbrv'].str.contains(\"FR\")== False].iloc[0::3,5]\n",
    "        if total_mig.shape[0] == 51:\n",
    "            total_mig = total_mig.reindex(State_name_index_abbr)\n",
    "            mig_table_inflow.iloc[:,int(19 + i)] = total_mig.values\n",
    "        else:\n",
    "            1#print('inflow')    \n",
    "#df = pd.read_sql_table(results_name[18], con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"irs_migration_state.sqlite\")\n",
    "\n",
    "outflow_1997_98 = pd.read_sql_query(\"SELECT * from outflow_1997_98\", con)\n",
    "inflow_2002_03 = pd.read_sql_query(\"SELECT * from inflow_2002_03\", con)\n",
    "outflow_1993_94 = pd.read_sql_query(\"SELECT * from outflow_1993_94\", con)\n",
    "outflow_1995_96 = pd.read_sql_query(\"SELECT * from outflow_1995_96\", con)\n",
    "outflow_2013_14 = pd.read_sql_query(\"SELECT * from outflow_2013_14\", con)\n",
    "outflow_2014_15 = pd.read_sql_query(\"SELECT * from outflow_2014_15\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outflow_1997_98_a = outflow_1997_98.pivot_table(index = 'st_orig_abbrv' , columns = 'st_dest_name', values = 'returns')\n",
    "outflow_1997_98_b = outflow_1997_98_a.iloc[:,outflow_1997_98_a.columns.str.contains(\"Migrants\") == False]\n",
    "outflow_1997_98_c = outflow_1997_98_b.reindex(State_abbrev.iloc[3:54,3].values)\n",
    "mig_table_outflow.iloc[:,28] = outflow_1997_98_c.fillna(0).values.sum(1)\n",
    "#take averages between years for the missing data\n",
    "for i in range(51):\n",
    "    if (mig_table_outflow.iloc[i,28] == 0):\n",
    "        mig_table_outflow.iloc[i,28] = int(mig_table_outflow.iloc[i,27] + mig_table_outflow.iloc[i,29])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inflow_2002_03_a = inflow_2002_03.pivot_table(index = 'st_dest_abbrv' , columns = 'st_orig_name', values = 'returns')\n",
    "inflow_2002_03_b = inflow_2002_03_a.iloc[:,inflow_2002_03_a.columns.str.contains(\"Migrants\") == False]\n",
    "inflow_2002_03_c = inflow_2002_03_b.reindex(State_abbrev.iloc[3:54,3].values)\n",
    "mig_table_inflow.iloc[:,33] = inflow_2002_03_c.fillna(0).values.sum(1)\n",
    "#take averages between years for the missing data\n",
    "for i in range(51):\n",
    "    if (mig_table_inflow.iloc[i,33] == 0):\n",
    "        mig_table_inflow.iloc[i,33] = int(mig_table_inflow.iloc[i,32] + mig_table_inflow.iloc[i,34])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outflow_1993_94_a = outflow_1993_94.pivot_table(index = 'st_orig_abbrv' , columns = 'st_dest_abbrv', values = 'returns')\n",
    "outflow_1993_94_b = outflow_1993_94_a.iloc[outflow_1993_94_a.index.str.contains(\"FR\") == False,outflow_1993_94_a.columns.str.contains(\"FR\") == False]\n",
    "outflow_1993_94_c = outflow_1993_94_b.reindex(State_abbrev.iloc[3:54,3].values)\n",
    "outflow_1993_94_c = outflow_1993_94_c.reindex(State_abbrev.iloc[3:54,3].values,axis =1)\n",
    "outflow_1993_94_c = outflow_1993_94_c.fillna(0)\n",
    "mig_table_nonmig.iloc[:,24] = outflow_1993_94_c.values.diagonal()\n",
    "for i in range(51):\n",
    "    if (mig_table_nonmig.iloc[i,24] == 0):\n",
    "        mig_table_nonmig.iloc[i,24] = int(mig_table_nonmig.iloc[i,23] + mig_table_nonmig.iloc[i,25])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outflow_1995_96_a = outflow_1995_96.pivot_table(index = 'st_orig_abbrv' , columns = 'st_dest_abbrv', values = 'returns')\n",
    "outflow_1995_96_b = outflow_1995_96_a.iloc[outflow_1995_96_a.index.str.contains(\"FR\") == False,outflow_1995_96_a.columns.str.contains(\"FR\") == False]\n",
    "outflow_1995_96_c = outflow_1995_96_b.reindex(State_abbrev.iloc[3:54,3].values)\n",
    "outflow_1995_96_c = outflow_1995_96_c.reindex(State_abbrev.iloc[3:54,3].values,axis =1)\n",
    "outflow_1995_96_c = outflow_1995_96_c.fillna(0)\n",
    "mig_table_nonmig.iloc[:,26] = outflow_1995_96_c.values.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outflow_2013_14_a = outflow_2013_14.pivot_table(index = 'st_orig_abbrv' , columns = 'st_dest_name', values = 'returns')\n",
    "outflow_2013_14_b = outflow_2013_14_a.iloc[:,outflow_2013_14_a.columns.str.contains(\"Non-migrants\") == True]\n",
    "outflow_2013_14_c = outflow_2013_14_b.reindex(State_abbrev.iloc[3:54,3].values)\n",
    "mig_table_nonmig.iloc[:,-3] = outflow_2013_14_c.fillna(0).values.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outflow_2014_15_a = outflow_2014_15.pivot_table(index = 'st_orig_abbrv' , columns = 'st_dest_name', values = 'returns')\n",
    "outflow_2014_15_b = outflow_2014_15_a.iloc[:,outflow_2014_15_a.columns.str.contains(\"Non-migrants\") == True]\n",
    "outflow_2014_15_c = outflow_2014_15_b.reindex(State_abbrev.iloc[3:54,3].values)\n",
    "mig_table_nonmig.iloc[:,-2] = outflow_2014_15_c.fillna(0).values.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mig_table_nonmig1 =  mig_table_nonmig.replace(0,1)\n",
    "mig_table_outflow_rate = mig_table_outflow/mig_table_nonmig1.values\n",
    "mig_table_inflow_rate = mig_table_inflow/mig_table_nonmig1.values\n",
    "mig_table_netflow = mig_table_inflow_rate - mig_table_outflow_rate.values\n",
    "mig_table_nonmig1_rate = mig_table_nonmig1 / mig_table_nonmig1.values.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Regional migration from the census bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('https://www2.census.gov/programs-surveys/demo/tables/geographic-mobility/time-series/historic/tab-a-2.xls',header = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Northeast'] =  df['Northeast'].astype(str).str.replace('*','').str.replace(',','')\n",
    "df['Midwest'] =  df['Midwest'].astype(str).str.replace('*','').str.replace(',','')\n",
    "df['South'] =  df['South'].astype(str).str.replace('*','').str.replace(',','')\n",
    "df['West'] =  df['West'].astype(str).str.replace('*','').str.replace(',','')\n",
    "df = df.sort_index(axis=0 ,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regional_mig_table_netflow = state_consumption.copy()\n",
    "as_list = regional_mig_table_netflow.index.tolist()\n",
    "as_list[0] = 'NORTHEAST'\n",
    "as_list[1] = 'MIDWEST'\n",
    "as_list[2] = 'SOUTH'\n",
    "as_list[3] = 'WEST'\n",
    "regional_mig_table_netflow.index = as_list\n",
    "regional_mig_table_netflow = regional_mig_table_netflow.drop(regional_mig_table_netflow.index[4:])\n",
    "regional_mig_table_netflow.iloc[:,:] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regional_mig_table_netflow.iloc[:,11:25] = df.iloc[11:105:7,1:].applymap(float).values.T\n",
    "int_migr95_on = df.iloc[111:-5:7,1:]\n",
    "int_migr95_on = int_migr95_on.reset_index()\n",
    "int_migr95_on = int_migr95_on.drop([5,6,16,18])\n",
    "int_migr95_on = int_migr95_on.iloc[:,1:]\n",
    "regional_mig_table_netflow.iloc[:,26:] = int_migr95_on.applymap(float).values.T\n",
    "regional_mig_table_netflow.iloc[:,25] = (regional_mig_table_netflow.iloc[:,24].values +\n",
    "                                         regional_mig_table_netflow.iloc[:,26].values)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wls = 0\n",
    "rng = list(range(0,51))#list(range(0,7)) + list(range(9, 28)) + list(range(29, 51)) #\n",
    "no_state = len(rng)\n",
    "time_length = 46\n",
    "start_period = 0\n",
    "original_data = 0 # set to 1 to use ASY original data with Population and CPI from here -rerun the whole code to restore\n",
    "#2 to use Census regions \n",
    "#3 to use European data\n",
    "migration = 0 # set to 1 to fix population_ratio at the initial ratio - upper bound, incorporates demographics too\n",
    "employment = 0 # set to 1 for using employment instead of population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (employment == 1 and (original_data == 0 or original_data == 1 or original_data == 2)):\n",
    "    Population1 = Population.copy()\n",
    "    Population = Employment.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Population_star = Population.copy()\n",
    "if (migration == 1):\n",
    "    Total_pop = Population_star.values.sum(0)\n",
    "    for i in range(Population_star.shape[1]):\n",
    "        Population_star.iloc[:,i] = Population_star.iloc[:,0]/Population_star.iloc[:,0].values.sum() * Total_pop[i]\n",
    "elif(migration ==2 and original_data == 0 ):\n",
    "    for i in range(Population_star.shape[1]):\n",
    "        Population_star.iloc[:,i] = Population_star.iloc[:,i] * (1 - mig_table_netflow.values[:,i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (original_data == 1):\n",
    "    non_federal_state_income = pd.read_excel('nch6.xls',header =1)\n",
    "    p = non_federal_state_income.pivot(index = 'State' , columns = 'Region')\n",
    "    non_federal_state_income = p.stack()\n",
    "    non_federal_state_income = non_federal_state_income.reset_index(level=1, drop=True)\n",
    "    non_federal_state_income  = non_federal_state_income.iloc[:,:-1]\n",
    "    disposable_state_income = pd.read_excel('nch11.xls',header =1)\n",
    "    p = disposable_state_income.pivot(index = 'State' , columns = 'Region')\n",
    "    disposable_state_income = p.stack()\n",
    "    disposable_state_income = disposable_state_income.reset_index(level=1, drop=True)\n",
    "    disposable_state_income  = disposable_state_income.iloc[:,:]\n",
    "    nominal_Industry_Total_GSP1 = pd.read_excel('nch14.xls',header =1)\n",
    "    nominal_Industry_Total_GSP1 = nominal_Industry_Total_GSP1.drop(nominal_Industry_Total_GSP1.index[-1])\n",
    "    p = nominal_Industry_Total_GSP1.pivot(index = 'State' , columns = 'Region')\n",
    "    nominal_Industry_Total_GSP1 = p.stack()\n",
    "    nominal_Industry_Total_GSP1 = nominal_Industry_Total_GSP1.reset_index(level=1, drop=True)\n",
    "    nominal_Industry_Total_GSP1 = nominal_Industry_Total_GSP1.iloc[:,:-1]\n",
    "    state_consumption = pd.read_excel('nch15.xls',header =1)\n",
    "    p = state_consumption.pivot(index = 'State' , columns = 'Region')\n",
    "    state_consumption = p.stack()\n",
    "    state_consumption = state_consumption.reset_index(level=1, drop=True)\n",
    "    state_consumption  = state_consumption.iloc[:,:]\n",
    "    names = Industry_Total_GSP.index\n",
    "    names_vector2 = [s + ' state total' for s in names] + [s for s in names] + [s + '*' for s in names] \n",
    "    SA4_table3 = SA4_table1.loc[names_vector2]\n",
    "    Population = SA4_table3.iloc[20::22].applymap(float)\n",
    "    Population = Population.iloc[:,:36]\n",
    "    cpi = GDP_deflator_annual.iloc[16:52,0].values\n",
    "    cpi_tile = np.tile(cpi,(no_state,1))\n",
    "    cpi_flat = cpi_tile.T.flatten()/ 100\n",
    "    time_length = 26\n",
    "    start_period = 0\n",
    "    pe_cons = state_consumption.iloc[rng,:]/np.sqrt(Population.iloc[rng,:].values)/cpi_tile * 1000000\n",
    "    pe_gsp = nominal_Industry_Total_GSP1.iloc[rng,:]/np.sqrt(Population.iloc[rng,:].values)/cpi_tile * 1000000\n",
    "    pe_income = non_federal_state_income.iloc[rng,:]/np.sqrt(Population.iloc[rng,:].values)/cpi_tile * 1000000\n",
    "    pe_disp_income = disposable_state_income.iloc[rng,:]/np.sqrt(Population.iloc[rng,:].values) /cpi_tile* 1000000\n",
    "    pe_cons = pe_cons.astype(float)\n",
    "    pe_gsp = pe_gsp.astype(float)\n",
    "    pe_income = pe_income.astype(float)\n",
    "    pe_disp_income = pe_disp_income.astype(float)\n",
    "    tostata_save = pe_cons.unstack().to_frame()\n",
    "    tostata_save.columns = ['Consumption']\n",
    "    tostata_save['GSP'] = pe_gsp.unstack().values\n",
    "    tostata_save['Inc'] = pe_income.unstack().values.T.flatten()\n",
    "    tostata_save['DInc'] = pe_disp_income.unstack().values.T.flatten()\n",
    "    tostata_save.to_csv('state.csv')\n",
    "    pe_cons = pe_cons.values.T.flatten()\n",
    "    pe_gsp = pe_gsp.values.T.flatten()\n",
    "    pe_income = pe_income.values.T.flatten()\n",
    "    pe_disp_income = pe_disp_income.values.T.flatten()\n",
    "elif(original_data == 2):\n",
    "    #Census bureau regional aggregation\n",
    "    NORTHEAST_index = np.asarray((6,19,21,29,30,32,38,39,45))\n",
    "    MIDWEST_index = np.asarray((13,14,15,16,22,23,27,34,35,41,49))\n",
    "    SOUTH_index = np.asarray((0,3,7,8,9,10,17,18,20,24,25,33,36,40,42,43,46,48))\n",
    "    WEST_index = np.asarray((2,4,5,12,26,28,31,37,44,47,50))\n",
    "    rng = list(range(0,4))\n",
    "    no_state = len(rng)\n",
    "    cpi = GDP_deflator_annual.iloc[22:69,0].values/ GDP_deflator_annual.iloc[43,0]\n",
    "    cpi_tile = np.tile(cpi,(no_state,1))\n",
    "    cpi_flat = cpi_tile.T.flatten()\n",
    "    region_cons = state_consumption.copy()\n",
    "    as_list = region_cons.index.tolist()\n",
    "    as_list[0] = 'NORTHEAST'\n",
    "    as_list[1] = 'MIDWEST'\n",
    "    as_list[2] = 'SOUTH'\n",
    "    as_list[3] = 'WEST'\n",
    "    region_cons.index = as_list\n",
    "    region_cons.loc['NORTHEAST',:] = state_consumption.iloc[NORTHEAST_index,:].values.sum(0)\n",
    "    region_cons.loc['MIDWEST',:] = state_consumption.iloc[MIDWEST_index,:].values.sum(0)\n",
    "    region_cons.loc['SOUTH',:] = state_consumption.iloc[SOUTH_index,:].values.sum(0)\n",
    "    region_cons.loc['WEST',:] = state_consumption.iloc[WEST_index,:].values.sum(0)\n",
    "    region_cons = region_cons.drop(region_cons.index[4:])\n",
    "    nominal_Industry_Total_GSP_regional = region_cons.copy() \n",
    "    nominal_Industry_Total_GSP_regional.loc['NORTHEAST',:] = nominal_Industry_Total_GSP1.iloc[NORTHEAST_index,:].values.sum(0)\n",
    "    nominal_Industry_Total_GSP_regional.loc['MIDWEST',:] = nominal_Industry_Total_GSP1.iloc[MIDWEST_index,:].values.sum(0)\n",
    "    nominal_Industry_Total_GSP_regional.loc['SOUTH',:] = nominal_Industry_Total_GSP1.iloc[SOUTH_index,:].values.sum(0)\n",
    "    nominal_Industry_Total_GSP_regional.loc['WEST',:] = nominal_Industry_Total_GSP1.iloc[WEST_index,:].values.sum(0)\n",
    "    non_federal_state_income_regional = region_cons.copy() \n",
    "    non_federal_state_income_regional.loc['NORTHEAST',:] = non_federal_state_income.iloc[NORTHEAST_index,:].values.sum(0)\n",
    "    non_federal_state_income_regional.loc['MIDWEST',:] = non_federal_state_income.iloc[MIDWEST_index,:].values.sum(0)\n",
    "    non_federal_state_income_regional.loc['SOUTH',:] = non_federal_state_income.iloc[SOUTH_index,:].values.sum(0)\n",
    "    non_federal_state_income_regional.loc['WEST',:] = non_federal_state_income.iloc[WEST_index,:].values.sum(0)\n",
    "    disposable_state_income_regional = region_cons.copy() \n",
    "    disposable_state_income_regional.loc['NORTHEAST',:] = disposable_state_income.iloc[NORTHEAST_index,:].values.sum(0)\n",
    "    disposable_state_income_regional.loc['MIDWEST',:] = disposable_state_income.iloc[MIDWEST_index,:].values.sum(0)\n",
    "    disposable_state_income_regional.loc['SOUTH',:] = disposable_state_income.iloc[SOUTH_index,:].values.sum(0)\n",
    "    disposable_state_income_regional.loc['WEST',:] = disposable_state_income.iloc[WEST_index,:].values.sum(0)\n",
    "    Population_regional = region_cons.copy()\n",
    "    \n",
    "    Population_regional.loc['NORTHEAST',:] = Population.iloc[NORTHEAST_index,:].values.sum(0)\n",
    "    Population_regional.loc['MIDWEST',:] = Population.iloc[MIDWEST_index,:].values.sum(0)\n",
    "    Population_regional.loc['SOUTH',:] = Population.iloc[SOUTH_index,:].values.sum(0)\n",
    "    Population_regional.loc['WEST',:] = Population.iloc[WEST_index,:].values.sum(0)\n",
    "    Population_star_regional = Population_regional.copy()\n",
    "    if (migration == 1):\n",
    "        Total_pop = Population_star_regional.values.sum(0)\n",
    "        for i in range(Population_star_regional.shape[1]):\n",
    "            Population_star_regional.iloc[:,i] = Population_star_regional.iloc[:,0]/Population_star_regional.iloc[:,0].values.sum() * Total_pop[i]\n",
    "    if(migration ==2):\n",
    "        for i in range(Population_star.shape[1]):\n",
    "            Population_star_regional.iloc[:,i] = Population_star_regional.iloc[:,i] - regional_mig_table_netflow.values[:,i]*1000\n",
    "    if wls == 0:\n",
    "        pe_cons = region_cons.iloc[rng,:].values.T.flatten()/(Population_regional.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_gsp = nominal_Industry_Total_GSP_regional.iloc[rng,:].values.T.flatten()/(Population_regional.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_gsp_star = nominal_Industry_Total_GSP_regional.iloc[rng,:].values.T.flatten()/(Population_star_regional.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_income =  non_federal_state_income_regional.iloc[rng,:].values.T.flatten()/(Population_regional.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_disp_income = disposable_state_income_regional.iloc[rng,:].values.T.flatten()/(Population_regional.iloc[rng,:].values.T.flatten()) /cpi_flat* 1000000\n",
    "        mu_gr = Population_regional.iloc[rng,:].values.T.flatten()/(Population_star_regional.iloc[rng,:].values.T.flatten())\n",
    "    else:\n",
    "        pe_cons = region_cons.iloc[rng,:].values.T.flatten()/np.sqrt(Population_regional.iloc[rng,:].values.T.flatten()) * 1000000 /cpi_flat\n",
    "        pe_gsp = nominal_Industry_Total_GSP_regional.iloc[rng,:].values.T.flatten()/np.sqrt(Population_regional.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_gsp_star = nominal_Industry_Total_GSP_regional.iloc[rng,:].values.T.flatten()/np.sqrt(Population_star_regional.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_income =  non_federal_state_income_regional.iloc[rng,:].values.T.flatten()/np.sqrt(Population_regional.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_disp_income = disposable_state_income_regional.iloc[rng,:].values.T.flatten()/np.sqrt(Population_regional.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        mu_gr = Population_regional.iloc[rng,:].values.T.flatten()/np.sqrt(Population_star_regional.iloc[rng,:].values.T.flatten())\n",
    "elif(original_data == 3):\n",
    "    EU_data_gdp = pd.read_csv('SNA_TABLE2_gdp.csv',header =0)\n",
    "    EU_data_pop = pd.read_csv('SNA_TABLE3_pop.csv',header =0)\n",
    "    EU_data_migr = pd.read_csv('EU_data_migration1.csv',header =5)\n",
    "    no_EU_members = 13\n",
    "    no_state = no_EU_members\n",
    "    rng = list(range(0,no_EU_members))\n",
    "    EU_data_gdp1 = EU_data_gdp.iloc[:,[1,3,7,14]]\n",
    "    EU_data_pop1 = EU_data_pop.iloc[:,[1,3,7,14]]\n",
    "    EU_data_gdp2 = EU_data_gdp1.pivot_table(values = 'Value', index=['Transaction','Country'],columns = 'Year')\n",
    "    EU_data_pop2 = EU_data_pop1.pivot_table(values = 'Value', index=['Transaction','Country'],columns = 'Year')\n",
    "    EU_cons = EU_data_gdp2.iloc[:no_EU_members,:].unstack().unstack()\n",
    "    EU_cons.index = EU_cons.index.droplevel(level=2)\n",
    "    EU_cons = EU_cons.to_frame().pivot_table(columns = 'Year',index = 'Country')\n",
    "    EU_cons.columns = EU_cons.columns.droplevel(level=0)\n",
    "    EU_GDP = EU_data_gdp2.iloc[no_EU_members:(2*no_EU_members),:].unstack().unstack()\n",
    "    EU_GDP.index = EU_GDP.index.droplevel(level=2)\n",
    "    EU_GDP = EU_GDP.to_frame().pivot_table(columns = 'Year',index = 'Country')\n",
    "    EU_GDP.columns = EU_GDP.columns.droplevel(level=0)\n",
    "    EU_GNDI = EU_data_gdp2.iloc[(2*no_EU_members):(3*no_EU_members),:].unstack().unstack()\n",
    "    EU_GNDI.index = EU_GNDI.index.droplevel(level=2)\n",
    "    EU_GNDI = EU_GNDI.to_frame().pivot_table(columns = 'Year',index = 'Country')\n",
    "    EU_GNDI.columns = EU_GNDI.columns.droplevel(level=0)\n",
    "    EU_GNI = EU_data_gdp2.iloc[(3*no_EU_members):(4*no_EU_members),:].unstack().unstack()\n",
    "    EU_GNI.index = EU_GNI.index.droplevel(level=2)\n",
    "    EU_GNI = EU_GNI.to_frame().pivot_table(columns = 'Year',index = 'Country')\n",
    "    EU_GNI.columns = EU_GNI.columns.droplevel(level=0)\n",
    "    EU_employment = EU_data_pop2.iloc[:no_EU_members,:].unstack().unstack()\n",
    "    EU_employment.index = EU_employment.index.droplevel(level=2)\n",
    "    EU_employment = EU_employment.to_frame().pivot_table(columns = 'Year',index = 'Country')\n",
    "    EU_employment.columns = EU_employment.columns.droplevel(level=0)\n",
    "    EU_pop = EU_data_pop2.iloc[no_EU_members:(2*no_EU_members),:].unstack().unstack()\n",
    "    EU_pop.index = EU_pop.index.droplevel(level=2)\n",
    "    EU_pop = EU_pop.to_frame().pivot_table(columns = 'Year',index = 'Country')\n",
    "    EU_pop.columns = EU_pop.columns.droplevel(level=0)\n",
    "    EU_pop_star = EU_pop.copy()\n",
    "    EU_data_migr = EU_data_migr.drop([EU_data_migr.index[14],EU_data_migr.index[0]])\n",
    "    EU_data_migr = EU_data_migr.drop(EU_data_migr.columns[1],axis =1)\n",
    "    EU_data_migr = EU_data_migr.set_index(['Year'])\n",
    "    EU_data_migr = EU_data_migr.replace('..',np.nan)\n",
    "    EU_data_migr = EU_data_migr.applymap(float)\n",
    "    EU_data_migr_rate = EU_data_migr / EU_pop.iloc[:,14:].values/1000\n",
    "    EU_pop1 = EU_pop.iloc[:,14:] * (np.isnan(EU_data_migr_rate.values )== 0 * np.ones(EU_data_migr_rate.values.shape))\n",
    "    EU_pop2 = EU_pop1 / EU_pop1.sum()\n",
    "    if (migration == 1):\n",
    "        Total_pop = EU_pop_star.values.sum(0)\n",
    "        for i in range(EU_pop_star.shape[1]):\n",
    "            EU_pop_star.iloc[:,i] = EU_pop_star.iloc[:,0]/EU_pop_star.iloc[:,0].values.sum() * Total_pop[i]\n",
    "    cpi = GDP_deflator_annual.iloc[23:69,0].values/ GDP_deflator_annual.iloc[43,0]\n",
    "    cpi_tile = np.tile(cpi,(no_EU_members,1))\n",
    "    cpi_flat = cpi_tile.T.flatten()\n",
    "    if wls == 0:\n",
    "        pe_cons = EU_cons.iloc[rng,:].values.T.flatten()/(EU_pop.iloc[rng,:].values.T.flatten())/cpi_flat * 1000\n",
    "        pe_gsp = EU_GDP.iloc[rng,:].values.T.flatten()/(EU_pop.iloc[rng,:].values.T.flatten())/cpi_flat * 1000\n",
    "        pe_gsp_star = EU_GDP.iloc[rng,:].values.T.flatten()/(EU_pop_star.iloc[rng,:].values.T.flatten())/cpi_flat * 1000\n",
    "        pe_income =  EU_GNI.iloc[rng,:].values.T.flatten()/(EU_pop.iloc[rng,:].values.T.flatten())/cpi_flat * 1000\n",
    "        pe_disp_income = EU_GNDI.iloc[rng,:].values.T.flatten()/(EU_pop.iloc[rng,:].values.T.flatten()) /cpi_flat* 1000\n",
    "        mu_gr = EU_pop.iloc[rng,:].values.T.flatten()/(EU_pop_star.iloc[rng,:].values.T.flatten())\n",
    "    else:\n",
    "        pe_cons = EU_cons.iloc[rng,:].values.T.flatten()/np.sqrt(EU_pop.iloc[rng,:].values.T.flatten()) * 1000 /cpi_flat\n",
    "        pe_gsp = EU_GDP.iloc[rng,:].values.T.flatten()/np.sqrt(EU_pop.iloc[rng,:].values.T.flatten())/cpi_flat * 1000\n",
    "        pe_gsp_star = EU_GDP.iloc[rng,:].values.T.flatten()/np.sqrt(EU_pop_star.iloc[rng,:].values.T.flatten())/cpi_flat * 1000\n",
    "        pe_income =  EU_GNI.iloc[rng,:].values.T.flatten()/np.sqrt(EU_pop.iloc[rng,:].values.T.flatten())/cpi_flat * 1000\n",
    "        pe_disp_income = EU_GNDI.iloc[rng,:].values.T.flatten()/np.sqrt(EU_pop.iloc[rng,:].values.T.flatten())/cpi_flat * 1\n",
    "        mu_gr = EU_pop.iloc[rng,:].values.T.flatten()/np.sqrt(EU_pop_star.iloc[rng,:].values.T.flatten())\n",
    "else:\n",
    "    cpi = GDP_deflator_annual.iloc[22:69,0].values/ GDP_deflator_annual.iloc[43,0]\n",
    "    cpi_tile = np.tile(cpi,(no_state,1))\n",
    "    cpi_flat = cpi_tile.T.flatten()\n",
    "    if wls == 0:\n",
    "        pe_cons = state_consumption.iloc[rng,:].values.T.flatten()/(Population.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_gsp = nominal_Industry_Total_GSP1.iloc[rng,:].values.T.flatten()/(Population.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_gsp_star = nominal_Industry_Total_GSP1.iloc[rng,:].values.T.flatten()/(Population_star.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_income = non_federal_state_income.iloc[rng,:].values.T.flatten()/(Population.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_disp_income = disposable_state_income.iloc[rng,:].values.T.flatten()/(Population.iloc[rng,:].values.T.flatten()) /cpi_flat* 1000000\n",
    "        mu_gr = Population.iloc[rng,:].values.T.flatten()/(Population_star.iloc[rng,:].values.T.flatten())\n",
    "    else:\n",
    "        pe_cons = state_consumption.iloc[rng,:].values.T.flatten()/np.sqrt(Population.iloc[rng,:].values.T.flatten()) * 1000000 /cpi_flat\n",
    "        pe_gsp = nominal_Industry_Total_GSP1.iloc[rng,:].values.T.flatten()/np.sqrt(Population.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_gsp_star = nominal_Industry_Total_GSP1.iloc[rng,:].values.T.flatten()/np.sqrt(Population_star.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_income = non_federal_state_income.iloc[rng,:].values.T.flatten()/np.sqrt(Population.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        pe_disp_income = disposable_state_income.iloc[rng,:].values.T.flatten()/np.sqrt(Population.iloc[rng,:].values.T.flatten())/cpi_flat * 1000000\n",
    "        mu_gr = np.sqrt(Population.iloc[rng,:].values.T.flatten())/np.sqrt(Population_star.iloc[rng,:].values.T.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pe_cons = np.log(pe_cons.astype(float))\n",
    "pe_gsp = np.log(pe_gsp.astype(float))\n",
    "pe_gsp_star = np.log(pe_gsp_star.astype(float))\n",
    "pe_income = np.log(pe_income.astype(float))\n",
    "pe_disp_income = np.log(pe_disp_income.astype(float))\n",
    "mu_gr = np.log(mu_gr.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the data to stata -EU:\n",
    "#stata_df = pd.DataFrame(data = ((EU_cons/ EU_pop.values)/cpi_flat.reshape(EU_pop.values.shape, order ='F') * 1000).unstack().to_frame())\n",
    "#stata_df = stata_df.assign(Output = ((EU_GDP/ EU_pop.values)/cpi_flat.reshape(EU_pop.values.shape, order ='F') * 1000).unstack().to_frame().values)\n",
    "#stata_df = stata_df.assign(Output_star = ((EU_GDP/ EU_pop_star.values)/cpi_flat.reshape(EU_pop.values.shape, order ='F') * 1000).unstack().to_frame().values)\n",
    "#stata_df = stata_df.assign(Income = ((EU_GNI/ EU_pop.values)/cpi_flat.reshape(EU_pop.values.shape, order ='F') * 1000).unstack().to_frame().values)\n",
    "#stata_df = stata_df.assign(Disposable_Income = ((EU_GNDI/ EU_pop.values)/cpi_flat.reshape(EU_pop.values.shape, order ='F') * 1000).unstack().to_frame().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-b919331fadbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnominal_Industry_Total_GSP1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mPopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mcpi_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOutput_star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnominal_Industry_Total_GSP1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mPopulation_star\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mcpi_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mstata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIncome\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_federal_state_income\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mPopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mcpi_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mstata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDisposable_Income\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisposable_state_income\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mPopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mcpi_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3612\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3613\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3614\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3616\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_frame'"
     ]
    }
   ],
   "source": [
    "# Save the data to stata-US state level:\n",
    "stata_df = pd.DataFrame(data = ((state_consumption.iloc[rng,:]/ Population.iloc[rng,:].values)/cpi_flat.reshape(Population.iloc[rng,:].values.shape, order ='F') * 1000).unstack().to_frame())\n",
    "stata_df = stata_df.assign(Output = ((nominal_Industry_Total_GSP1.iloc[rng,:]/ Population.iloc[rng,:].values)/cpi_flat.reshape(Population.iloc[rng,:].values.shape, order ='F') * 1000).unstack().to_frame().values)\n",
    "stata_df = stata_df.assign(Output_star = ((nominal_Industry_Total_GSP1.iloc[rng,:]/ Population_star.iloc[rng,:].values)/cpi_flat.reshape(Population.iloc[rng,:].values.shape, order ='F') * 1000).unstack().to_frame().values)\n",
    "stata_df = stata_df.assign(Income = ((non_federal_state_income.iloc[rng,:]/ Population.iloc[rng,:].values)/cpi_flat.reshape(Population.iloc[rng,:].values.shape, order ='F') * 1000).unstack().to_frame().values)\n",
    "stata_df = stata_df.assign(Disposable_Income = ((disposable_state_income.iloc[rng,:]/ Population.iloc[rng,:].values)/cpi_flat.reshape(Population.iloc[rng,:].values.shape, order ='F') * 1000).unstack().to_frame().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the data to stata-US regional level:\n",
    "#stata_df = pd.DataFrame(data = ((region_cons.iloc[rng,:]/ Population_regional.iloc[rng,:].values)/cpi_flat.reshape(Population_regional.iloc[rng,:].values.shape, order ='F') * 1000).unstack().to_frame())\n",
    "#stata_df = stata_df.assign(Output = ((nominal_Industry_Total_GSP_regional.iloc[rng,:]/ Population_regional.iloc[rng,:].values)/cpi_flat.reshape(Population_regional.iloc[rng,:].values.shape, order ='F') * 1000).unstack().to_frame().values)\n",
    "#stata_df = stata_df.assign(Output_star = ((nominal_Industry_Total_GSP_regional.iloc[rng,:]/ Population_star_regional.iloc[rng,:].values)/cpi_flat.reshape(Population_regional.iloc[rng,:].values.shape, order ='F') * 1000).unstack().to_frame().values)\n",
    "#stata_df = stata_df.assign(Income = ((non_federal_state_income_regional.iloc[rng,:]/ Population_regional.iloc[rng,:].values)/cpi_flat.reshape(Population_regional.iloc[rng,:].values.shape, order ='F') * 1000).unstack().to_frame().values)\n",
    "#stata_df = stata_df.assign(Disposable_Income = ((disposable_state_income_regional.iloc[rng,:]/ Population_regional.iloc[rng,:].values)/cpi_flat.reshape(Population_regional.iloc[rng,:].values.shape, order ='F') * 1000).unstack().to_frame().values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stata_df.columns = ['Consumption','Output','Output_star','Income','Disposable_Income']\n",
    "stata_df.reset_index(inplace=True)\n",
    "#stata_df.level_1 = stata_df.level_1.str.encode('latin-1')\n",
    "#stata_df.level_0 = stata_df.level_0.str.encode('latin-1')\n",
    "#list(stata_df.select_dtypes(include=['object']).columns)\n",
    "#stata_df['level_0'] = stata_df['level_0'].astype('int64')\n",
    "#stata_df['level_1'] = stata_df['level_1'].astype(str)\n",
    "#stata_df['Consumption'] = stata_df['Consumption'].astype('float')\n",
    "#stata_df['Output'] = stata_df['Output'].astype('float')\n",
    "#stata_df['Output_star'] = stata_df['Output_star'].astype('float')\n",
    "#stata_df['Income'] = stata_df['Income'].astype('float')\n",
    "#stata_df['Disposable_Income'] = stata_df['Disposable_Income'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stata_df.to_stata('EU_data.dta')\n",
    "stata_df.to_stata('US_state_data.dta')\n",
    "#stata_df.to_stata('US_regional_data.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(np.arange(1988,2015),(mig_table_inflow_rate.iloc[:,19:-1] * mig_table_nonmig1_rate.iloc[:,19:-1].values).sum(),color='blue',\n",
    "        linewidth=3, markersize=12,label = 'US')\n",
    "ax.plot(np.arange(1988,2015),(EU_data_migr_rate * EU_pop2.values).sum().values[4:-1],color='red',  linestyle='dotted',\n",
    "        linewidth=3, markersize=12,label = 'EU')\n",
    "ax.legend(loc=\"center right\",fontsize=24)\n",
    "tikz_save('migr_rate_EUS.tex', figureheight='\\\\figureheight',figurewidth='\\\\figurewidth')\n",
    "fig.savefig('migr_rate_EUS.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tikz_save?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Output</th>\n",
       "      <th>Output_star</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">1969</th>\n",
       "      <th>Alabama</th>\n",
       "      <td>8.68157</td>\n",
       "      <td>10.532550</td>\n",
       "      <td>10.532550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>12.1579</td>\n",
       "      <td>19.840106</td>\n",
       "      <td>19.840106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>11.0621</td>\n",
       "      <td>13.724303</td>\n",
       "      <td>13.724303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>9.80962</td>\n",
       "      <td>10.017137</td>\n",
       "      <td>10.017137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>12.8676</td>\n",
       "      <td>17.321310</td>\n",
       "      <td>17.321310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>11.2209</td>\n",
       "      <td>13.708082</td>\n",
       "      <td>13.708082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Connecticut</th>\n",
       "      <td>11.996</td>\n",
       "      <td>15.975278</td>\n",
       "      <td>15.975278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delaware</th>\n",
       "      <td>12.2021</td>\n",
       "      <td>18.684362</td>\n",
       "      <td>18.684362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>District of Columbia</th>\n",
       "      <td>15.2307</td>\n",
       "      <td>33.847921</td>\n",
       "      <td>33.847921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Florida</th>\n",
       "      <td>11.7348</td>\n",
       "      <td>12.980476</td>\n",
       "      <td>12.980476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Georgia</th>\n",
       "      <td>10.0715</td>\n",
       "      <td>12.560851</td>\n",
       "      <td>12.560851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hawaii</th>\n",
       "      <td>11.9158</td>\n",
       "      <td>16.978427</td>\n",
       "      <td>16.978427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idaho</th>\n",
       "      <td>11.0593</td>\n",
       "      <td>11.619451</td>\n",
       "      <td>11.619451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>12.1758</td>\n",
       "      <td>17.485240</td>\n",
       "      <td>17.485240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indiana</th>\n",
       "      <td>11.3666</td>\n",
       "      <td>14.916386</td>\n",
       "      <td>14.916386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iowa</th>\n",
       "      <td>12.4263</td>\n",
       "      <td>13.606846</td>\n",
       "      <td>13.606846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>10.8974</td>\n",
       "      <td>12.761375</td>\n",
       "      <td>12.761375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kentucky</th>\n",
       "      <td>9.24263</td>\n",
       "      <td>12.828080</td>\n",
       "      <td>12.828080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Louisiana</th>\n",
       "      <td>9.77571</td>\n",
       "      <td>12.966416</td>\n",
       "      <td>12.966416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maine</th>\n",
       "      <td>10.216</td>\n",
       "      <td>11.417021</td>\n",
       "      <td>11.417021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maryland</th>\n",
       "      <td>11.3396</td>\n",
       "      <td>14.370104</td>\n",
       "      <td>14.370104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Massachusetts</th>\n",
       "      <td>11.7533</td>\n",
       "      <td>15.613419</td>\n",
       "      <td>15.613419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>11.6692</td>\n",
       "      <td>17.706758</td>\n",
       "      <td>17.706758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minnesota</th>\n",
       "      <td>11.4556</td>\n",
       "      <td>14.498990</td>\n",
       "      <td>14.498990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mississippi</th>\n",
       "      <td>8.29872</td>\n",
       "      <td>9.180352</td>\n",
       "      <td>9.180352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>11.1328</td>\n",
       "      <td>14.420237</td>\n",
       "      <td>14.420237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montana</th>\n",
       "      <td>11.3328</td>\n",
       "      <td>12.374870</td>\n",
       "      <td>12.374870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nebraska</th>\n",
       "      <td>11.9286</td>\n",
       "      <td>13.698224</td>\n",
       "      <td>13.698224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>14.293</td>\n",
       "      <td>19.037821</td>\n",
       "      <td>19.037821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>11.3918</td>\n",
       "      <td>12.421336</td>\n",
       "      <td>12.421336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">2015</th>\n",
       "      <th>Massachusetts</th>\n",
       "      <td>35.7826</td>\n",
       "      <td>44.118811</td>\n",
       "      <td>44.118811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>27.1184</td>\n",
       "      <td>29.098235</td>\n",
       "      <td>29.098235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minnesota</th>\n",
       "      <td>30.7587</td>\n",
       "      <td>36.556836</td>\n",
       "      <td>36.556836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mississippi</th>\n",
       "      <td>21.9628</td>\n",
       "      <td>21.717540</td>\n",
       "      <td>21.717540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>25.8309</td>\n",
       "      <td>29.541550</td>\n",
       "      <td>29.541550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montana</th>\n",
       "      <td>29.0189</td>\n",
       "      <td>27.239414</td>\n",
       "      <td>27.239414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nebraska</th>\n",
       "      <td>27.9034</td>\n",
       "      <td>36.738823</td>\n",
       "      <td>36.738823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>25.1802</td>\n",
       "      <td>29.997983</td>\n",
       "      <td>29.997983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>33.1579</td>\n",
       "      <td>34.240168</td>\n",
       "      <td>34.240168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Jersey</th>\n",
       "      <td>34.3592</td>\n",
       "      <td>38.730883</td>\n",
       "      <td>38.730883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>25.8566</td>\n",
       "      <td>27.481477</td>\n",
       "      <td>27.481477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>33.8418</td>\n",
       "      <td>44.891332</td>\n",
       "      <td>44.891332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Carolina</th>\n",
       "      <td>23.7301</td>\n",
       "      <td>30.537660</td>\n",
       "      <td>30.537660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Dakota</th>\n",
       "      <td>36.2395</td>\n",
       "      <td>45.314429</td>\n",
       "      <td>45.314429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>26.2614</td>\n",
       "      <td>32.091466</td>\n",
       "      <td>32.091466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oklahoma</th>\n",
       "      <td>23.6722</td>\n",
       "      <td>29.505979</td>\n",
       "      <td>29.505979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>28.2229</td>\n",
       "      <td>32.990723</td>\n",
       "      <td>32.990723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pennsylvania</th>\n",
       "      <td>28.5575</td>\n",
       "      <td>33.959461</td>\n",
       "      <td>33.959461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhode Island</th>\n",
       "      <td>29.4646</td>\n",
       "      <td>32.328020</td>\n",
       "      <td>32.328020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Carolina</th>\n",
       "      <td>23.8272</td>\n",
       "      <td>25.285183</td>\n",
       "      <td>25.285183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>27.6563</td>\n",
       "      <td>33.646647</td>\n",
       "      <td>33.646647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tennessee</th>\n",
       "      <td>23.519</td>\n",
       "      <td>29.443508</td>\n",
       "      <td>29.443508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>25.6495</td>\n",
       "      <td>36.019908</td>\n",
       "      <td>36.019908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>24.7024</td>\n",
       "      <td>30.572289</td>\n",
       "      <td>30.572289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vermont</th>\n",
       "      <td>33.5712</td>\n",
       "      <td>29.677236</td>\n",
       "      <td>29.677236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>29.2081</td>\n",
       "      <td>35.302994</td>\n",
       "      <td>35.302994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>30.4593</td>\n",
       "      <td>38.231938</td>\n",
       "      <td>38.231938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>24.295</td>\n",
       "      <td>24.442819</td>\n",
       "      <td>24.442819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>27.652</td>\n",
       "      <td>32.063562</td>\n",
       "      <td>32.063562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>32.539</td>\n",
       "      <td>41.206809</td>\n",
       "      <td>41.206809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2397 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0     Output  Output_star\n",
       "     state                                                \n",
       "1969 Alabama               8.68157  10.532550    10.532550\n",
       "     Alaska                12.1579  19.840106    19.840106\n",
       "     Arizona               11.0621  13.724303    13.724303\n",
       "     Arkansas              9.80962  10.017137    10.017137\n",
       "     California            12.8676  17.321310    17.321310\n",
       "     Colorado              11.2209  13.708082    13.708082\n",
       "     Connecticut            11.996  15.975278    15.975278\n",
       "     Delaware              12.2021  18.684362    18.684362\n",
       "     District of Columbia  15.2307  33.847921    33.847921\n",
       "     Florida               11.7348  12.980476    12.980476\n",
       "     Georgia               10.0715  12.560851    12.560851\n",
       "     Hawaii                11.9158  16.978427    16.978427\n",
       "     Idaho                 11.0593  11.619451    11.619451\n",
       "     Illinois              12.1758  17.485240    17.485240\n",
       "     Indiana               11.3666  14.916386    14.916386\n",
       "     Iowa                  12.4263  13.606846    13.606846\n",
       "     Kansas                10.8974  12.761375    12.761375\n",
       "     Kentucky              9.24263  12.828080    12.828080\n",
       "     Louisiana             9.77571  12.966416    12.966416\n",
       "     Maine                  10.216  11.417021    11.417021\n",
       "     Maryland              11.3396  14.370104    14.370104\n",
       "     Massachusetts         11.7533  15.613419    15.613419\n",
       "     Michigan              11.6692  17.706758    17.706758\n",
       "     Minnesota             11.4556  14.498990    14.498990\n",
       "     Mississippi           8.29872   9.180352     9.180352\n",
       "     Missouri              11.1328  14.420237    14.420237\n",
       "     Montana               11.3328  12.374870    12.374870\n",
       "     Nebraska              11.9286  13.698224    13.698224\n",
       "     Nevada                 14.293  19.037821    19.037821\n",
       "     New Hampshire         11.3918  12.421336    12.421336\n",
       "...                            ...        ...          ...\n",
       "2015 Massachusetts         35.7826  44.118811    44.118811\n",
       "     Michigan              27.1184  29.098235    29.098235\n",
       "     Minnesota             30.7587  36.556836    36.556836\n",
       "     Mississippi           21.9628  21.717540    21.717540\n",
       "     Missouri              25.8309  29.541550    29.541550\n",
       "     Montana               29.0189  27.239414    27.239414\n",
       "     Nebraska              27.9034  36.738823    36.738823\n",
       "     Nevada                25.1802  29.997983    29.997983\n",
       "     New Hampshire         33.1579  34.240168    34.240168\n",
       "     New Jersey            34.3592  38.730883    38.730883\n",
       "     New Mexico            25.8566  27.481477    27.481477\n",
       "     New York              33.8418  44.891332    44.891332\n",
       "     North Carolina        23.7301  30.537660    30.537660\n",
       "     North Dakota          36.2395  45.314429    45.314429\n",
       "     Ohio                  26.2614  32.091466    32.091466\n",
       "     Oklahoma              23.6722  29.505979    29.505979\n",
       "     Oregon                28.2229  32.990723    32.990723\n",
       "     Pennsylvania          28.5575  33.959461    33.959461\n",
       "     Rhode Island          29.4646  32.328020    32.328020\n",
       "     South Carolina        23.8272  25.285183    25.285183\n",
       "     South Dakota          27.6563  33.646647    33.646647\n",
       "     Tennessee              23.519  29.443508    29.443508\n",
       "     Texas                 25.6495  36.019908    36.019908\n",
       "     Utah                  24.7024  30.572289    30.572289\n",
       "     Vermont               33.5712  29.677236    29.677236\n",
       "     Virginia              29.2081  35.302994    35.302994\n",
       "     Washington            30.4593  38.231938    38.231938\n",
       "     West Virginia          24.295  24.442819    24.442819\n",
       "     Wisconsin              27.652  32.063562    32.063562\n",
       "     Wyoming                32.539  41.206809    41.206809\n",
       "\n",
       "[2397 rows x 3 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-ee06029930f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_federal_state_income\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mPopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mcpi_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3612\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3613\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3614\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3616\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_frame'"
     ]
    }
   ],
   "source": [
    "((non_federal_state_income.iloc[rng,:]/ Population.iloc[rng,:].values)/cpi_flat.reshape(Population.iloc[rng,:].values.shape, order ='F') * 1000).unstack().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demean(vari):\n",
    "    ones_vari = np.ones(vari.shape)\n",
    "    return vari - ones_vari @ np.linalg.solve(ones_vari.T @  ones_vari, ones_vari.T @  vari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_length = 20\n",
    "start_period = 0\n",
    "def time_horizon_gls(start_period,time_length):\n",
    "    def OLS(X,y,Omega_inv = np.eye(time_length  * no_state,time_length  * no_state)):\n",
    "        beta = np.linalg.solve(X.T @ Omega_inv @  X, X.T @ Omega_inv  @ y)\n",
    "        y_hat = X @ beta\n",
    "        var = np.sum((y_hat - y)**2)/(time_length  * no_state - 1 )\n",
    "        var_beta = var *  np.linalg.inv(X.T @ X)\n",
    "        return beta , y_hat , var , var_beta\n",
    "    time_fixed_effects = np.kron(np.eye(time_length,time_length),np.ones((no_state,1)))\n",
    "    shape_vec = pe_cons[no_state:].shape[0]\n",
    "    d_pe_cons = np.reshape(pe_cons[no_state:] - pe_cons[:-no_state],(shape_vec,1))\n",
    "    d_pe_gsp = np.reshape(pe_gsp[no_state:] - pe_gsp[:-no_state],(shape_vec,1))\n",
    "    d_pe_income = np.reshape(pe_income[no_state:] - pe_income[:-no_state],(shape_vec,1))\n",
    "    d_pe_disp_income = np.reshape(pe_disp_income[no_state:] - pe_disp_income[:-no_state],(shape_vec,1))\n",
    "    d_pe_cons = demean(d_pe_cons)\n",
    "    d_pe_gsp = demean(d_pe_gsp)\n",
    "    d_pe_income = demean(d_pe_income)\n",
    "    d_pe_disp_income = demean(d_pe_disp_income)\n",
    "    y_1 =  d_pe_income[(start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    X = d_pe_gsp[(start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,X_mean,var,var_beta = OLS(time_fixed_effects,X)\n",
    "    X = X - X_mean\n",
    "    X = demean(X)\n",
    "    y_1 = demean(y_1)\n",
    "    beta_mean,y_1_mean,var,var_beta = OLS(time_fixed_effects,y_1)\n",
    "    y_1 = y_1 - y_1_mean\n",
    "    beta , y_1_hat , var , var_beta = OLS(X,y_1)\n",
    "    beta_capital = beta[0]\n",
    "    beta_capital,np.sqrt(var_beta[0,0])\n",
    "    res_1 = y_1_hat - y_1\n",
    "    w_1 = res_1**2\n",
    "    w_1_diag = w_1.copy()\n",
    "    for i in range(no_state):\n",
    "        w_1_diag[i::no_state] =  np.sqrt(1/(w_1[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_1_diag)\n",
    "    X_1 = Omega_inv @ X\n",
    "    y_1g = Omega_inv @ y_1\n",
    "    res_1g = Omega_inv @ res_1\n",
    "    #Federal redistribution\n",
    "    y_2 =  d_pe_disp_income[\n",
    "       (start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,y_2_mean,var,var_beta = OLS(time_fixed_effects,y_2)\n",
    "    y_2 = y_2 - y_2_mean\n",
    "    y_2 = demean(y_2)\n",
    "    beta , y_2_hat , var , var_beta = OLS(X,y_2)\n",
    "    beta_federal = beta[0]\n",
    "    beta_federal,np.sqrt(var_beta[0,0])\n",
    "    res_2 = y_2_hat - y_2\n",
    "    w_2 = res_2**2\n",
    "    w_2_diag = w_2.copy()\n",
    "    for i in range(no_state):\n",
    "        w_2_diag[i::no_state] =  np.sqrt(1/(w_2[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_2_diag)\n",
    "    X_2 = Omega_inv @ X\n",
    "    y_2g = Omega_inv @ y_2\n",
    "    res_2g = Omega_inv @ res_2\n",
    "    #Credit market\n",
    "    y_3 =  d_pe_cons[\n",
    "        (start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,y_3_mean,var,var_beta = OLS(time_fixed_effects,y_3)\n",
    "    y_3 = y_3 - y_3_mean\n",
    "    y_3 = demean(y_3)\n",
    "    beta , y_3_hat , var , var_beta = OLS(X,y_3)\n",
    "    beta_credit = beta[0]\n",
    "    beta_credit,np.sqrt(var_beta[0,0])\n",
    "    res_3 = y_3_hat - y_3\n",
    "    w_3 = res_3**2\n",
    "    w_3_diag = w_3.copy()\n",
    "    for i in range(no_state):\n",
    "        w_3_diag[i::no_state] =  np.sqrt(1/(w_3[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_3_diag)\n",
    "    X_3 = Omega_inv @ X\n",
    "    y_3g = Omega_inv @ y_3\n",
    "    res_3g = Omega_inv @ res_3\n",
    "    #FGLS\n",
    "    X_gls = sp.linalg.block_diag(X_1,X_2,X_3)\n",
    "    y_stack = np.concatenate((np.concatenate((y_1g,y_2g),axis =0),y_3g),axis =0)\n",
    "    Omega_sur = np.ones((3,3))\n",
    "    Omega_sur[0,1] = np.mean(res_1g * res_2g) \n",
    "    Omega_sur[0,2] = np.mean(res_1g * res_3g) \n",
    "    Omega_sur[1,2] = np.mean(res_3g * res_2g) \n",
    "    Omega_sur[2,1] = Omega_sur[1,2]\n",
    "    Omega_sur[2,0] = Omega_sur[0,2]\n",
    "    Omega_sur[1,0] = Omega_sur[0,1]\n",
    "    Omega_sur_inv = sp.linalg.inv(Omega_sur)\n",
    "    Omega_inv = np.kron(Omega_sur_inv, np.eye(time_length  * no_state,time_length  * no_state))\n",
    "    beta , y_3_hat , var , var_beta = OLS(X_gls,y_stack,Omega_inv)\n",
    "    eta = np.zeros((4,3))\n",
    "    eta[0,0] = -1\n",
    "    eta[1,0] = 1\n",
    "    eta[1,1] = -1\n",
    "    eta[2,1] = 1\n",
    "    eta[2,2] = -1\n",
    "    eta[3,2] = 1\n",
    "    var_eta = np.sqrt(np.diag(eta @ var_beta @ eta.T))\n",
    "    eta = eta @ beta\n",
    "    eta[0] = eta[0] +1\n",
    "    return eta,var_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_horizon_gls(start_period,35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_period_grid = 45 - time_length\n",
    "smoothing_grid = np.ones((4,start_period_grid))\n",
    "time_grid = np.ones((start_period_grid,1))\n",
    "for i in range(start_period_grid):\n",
    "    time_grid[i,0] = i + 1970 + int(time_length/2)\n",
    "    coeff,var = time_horizon_gls(i,time_length)\n",
    "    smoothing_grid[:,i] = coeff[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(time_grid,smoothing_grid.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = smoothing_grid\n",
    "data_shape = np.shape(smoothing_grid)\n",
    "\n",
    "# Take negative and positive data apart and cumulate\n",
    "def get_cumulated_array(data, **kwargs):\n",
    "    cum = data.clip(**kwargs)\n",
    "    cum = np.cumsum(cum, axis=0)\n",
    "    d = np.zeros(np.shape(data))\n",
    "    d[1:] = cum[:-1]\n",
    "    return d  \n",
    "\n",
    "cumulated_data = get_cumulated_array(data, min=0)\n",
    "cumulated_data_neg = get_cumulated_array(data, max=0)\n",
    "\n",
    "# Re-merge negative and positive data.\n",
    "row_mask = (data<0)\n",
    "cumulated_data[row_mask] = cumulated_data_neg[row_mask]\n",
    "data_stack = cumulated_data\n",
    "\n",
    "cols = [\"b\", \"g\", \"r\", \"c\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111)\n",
    "labels = (\"Capital\",\"Federal\",\"Credit\",\"Unsmoothed\")\n",
    "for i in np.arange(0, data_shape[0]):\n",
    "    ax.bar(time_grid[:,0], data[i], label=labels[i], bottom=data_stack[i], color=cols[i],)\n",
    "ax.legend(loc=\"best\",fontsize=10)\n",
    "fig.savefig('region.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (employment == 1 and (original_data == 0 or original_data == 1)):\n",
    "    Population = Population1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_horizon_gls4(start_period,time_length):\n",
    "    def OLS(X,y,Omega_inv = np.eye(time_length  * no_state,time_length  * no_state)):\n",
    "        beta = np.linalg.solve(X.T @ Omega_inv @  X, X.T @ Omega_inv  @ y)\n",
    "        y_hat = X @ beta\n",
    "        var = np.sum((y_hat - y)**2)/(time_length  * no_state - 1 )\n",
    "        var_beta = var *  np.linalg.inv(X.T @ X)\n",
    "        return beta , y_hat , var , var_beta\n",
    "    time_fixed_effects = np.kron(np.eye(time_length,time_length),np.ones((no_state,1)))\n",
    "    shape_vec = pe_cons[no_state:].shape[0]\n",
    "    d_pe_cons = np.reshape(pe_cons[no_state:] - pe_cons[:-no_state],(shape_vec,1))\n",
    "    d_pe_gsp = np.reshape(pe_gsp[no_state:] - pe_gsp[:-no_state],(shape_vec,1))\n",
    "    d_pe_gsp_star = np.reshape(pe_gsp_star[no_state:] - pe_gsp_star[:-no_state],(shape_vec,1))\n",
    "    d_pe_income = np.reshape(pe_income[no_state:] - pe_income[:-no_state],(shape_vec,1))\n",
    "    d_pe_disp_income = np.reshape(pe_disp_income[no_state:] - pe_disp_income[:-no_state],(shape_vec,1))\n",
    "    d_mu_gr = np.reshape(mu_gr[no_state:] - mu_gr[:-no_state],(shape_vec,1))\n",
    "    \n",
    "    d_pe_cons = demean(d_pe_cons)\n",
    "    d_pe_gsp = demean(d_pe_gsp)\n",
    "    d_pe_gsp_star = demean(d_pe_gsp_star)\n",
    "    d_pe_income = demean(d_pe_income)\n",
    "    d_pe_disp_income = demean(d_pe_disp_income)\n",
    "    d_mu_gr = demean(d_mu_gr)\n",
    "    \n",
    "    y_1 =  d_pe_income[(start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    X = d_pe_gsp_star[(start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,X_mean,var,var_beta = OLS(time_fixed_effects,X)\n",
    "    X = X - X_mean\n",
    "    X = demean(X)\n",
    "    y_1 = demean(y_1)\n",
    "    beta_mean,y_1_mean,var,var_beta = OLS(time_fixed_effects,y_1)\n",
    "    y_1 = y_1 - y_1_mean\n",
    "    beta , y_1_hat , var , var_beta = OLS(X,y_1)\n",
    "    res_1 = y_1_hat - y_1\n",
    "    w_1 = res_1**2\n",
    "    w_1_diag = w_1.copy()\n",
    "    for i in range(no_state):\n",
    "        w_1_diag[i::no_state] =  np.sqrt(1/(w_1[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_1_diag)\n",
    "    X_1 = Omega_inv @ X\n",
    "    y_1g = Omega_inv @ y_1\n",
    "    res_1g = Omega_inv @ res_1\n",
    "    #Federal redistribution\n",
    "    y_2 =  d_pe_disp_income[\n",
    "       (start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,y_2_mean,var,var_beta = OLS(time_fixed_effects,y_2)\n",
    "    y_2 = y_2 - y_2_mean\n",
    "    y_2 = demean(y_2)\n",
    "    beta , y_2_hat , var , var_beta = OLS(X,y_2)\n",
    "    res_2 = y_2_hat - y_2\n",
    "    w_2 = res_2**2\n",
    "    w_2_diag = w_2.copy()\n",
    "    for i in range(no_state):\n",
    "        w_2_diag[i::no_state] =  np.sqrt(1/(w_2[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_2_diag)\n",
    "    X_2 = Omega_inv @ X\n",
    "    y_2g = Omega_inv @ y_2\n",
    "    res_2g = Omega_inv @ res_2\n",
    "    #Credit market\n",
    "    y_3 =  d_pe_cons[\n",
    "        (start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,y_3_mean,var,var_beta = OLS(time_fixed_effects,y_3)\n",
    "    y_3 = y_3 - y_3_mean\n",
    "    y_3 = demean(y_3)\n",
    "    beta , y_3_hat , var , var_beta = OLS(X,y_3)\n",
    "    res_3 = y_3_hat - y_3\n",
    "    w_3 = res_3**2\n",
    "    w_3_diag = w_3.copy()\n",
    "    for i in range(no_state):\n",
    "        w_3_diag[i::no_state] =  np.sqrt(1/(w_3[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_3_diag)\n",
    "    X_3 = Omega_inv @ X\n",
    "    y_3g = Omega_inv @ y_3\n",
    "    res_3g = Omega_inv @ res_3\n",
    "    # Migration\n",
    "    y_4 =  d_mu_gr[\n",
    "        (start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,y_4_mean,var,var_beta = OLS(time_fixed_effects,y_4)\n",
    "    y_4 = y_4 - y_4_mean\n",
    "    y_4 = demean(y_4)\n",
    "    beta , y_4_hat , var , var_beta = OLS(X,y_4)\n",
    "    res_4 = y_4_hat - y_4\n",
    "    w_4 = res_4**2\n",
    "    w_4_diag = w_4.copy()\n",
    "    for i in range(no_state):\n",
    "        w_4_diag[i::no_state] =  np.sqrt(1/(w_4[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_4_diag)\n",
    "    X_4 = Omega_inv @ X\n",
    "    y_4g = Omega_inv @ y_4\n",
    "    res_4g = Omega_inv @ res_4\n",
    "    # GDP_star\n",
    "    y_5 =  d_pe_gsp[\n",
    "        (start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,y_5_mean,var,var_beta = OLS(time_fixed_effects,y_5)\n",
    "    y_5 = y_5 - y_5_mean\n",
    "    y_5 = demean(y_5)\n",
    "    beta , y_5_hat , var , var_beta = OLS(X,y_5)\n",
    "    res_5 = y_5_hat - y_5\n",
    "    w_5 = res_5**2\n",
    "    w_5_diag = w_5.copy()\n",
    "    for i in range(no_state):\n",
    "        w_5_diag[i::no_state] =  np.sqrt(1/(w_5[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_5_diag)\n",
    "    X_5 = Omega_inv @ X\n",
    "    y_5g = Omega_inv @ y_5\n",
    "    res_5g = Omega_inv @ res_5\n",
    "    #FGLS\n",
    "    X_gls = sp.linalg.block_diag(X_1,X_2,X_3,X_5)\n",
    "    y_stack = np.concatenate((np.concatenate((np.concatenate((y_1g,y_2g),axis =0),y_3g),axis =0),y_5g),axis =0)\n",
    "    Omega_sur = np.ones((4,4))\n",
    "    Omega_sur[0,1] = np.mean(res_1g * res_2g) \n",
    "    Omega_sur[0,2] = np.mean(res_1g * res_3g)\n",
    "    Omega_sur[0,3] = np.mean(res_1g * res_5g)\n",
    "    Omega_sur[3,0] = Omega_sur[0,3]\n",
    "    Omega_sur[2,0] = Omega_sur[0,2]\n",
    "    Omega_sur[1,0] = Omega_sur[0,1]\n",
    "    \n",
    "    \n",
    "    Omega_sur[1,2] = np.mean(res_3g * res_2g) \n",
    "    Omega_sur[1,3] = np.mean(res_5g * res_2g) \n",
    "\n",
    "    Omega_sur[2,1] = Omega_sur[1,2]\n",
    "    Omega_sur[3,1] = Omega_sur[1,3]\n",
    "\n",
    "    \n",
    "    Omega_sur[2,3] = np.mean(res_5g * res_3g) \n",
    "\n",
    "    Omega_sur[3,2] = Omega_sur[2,3]\n",
    "\n",
    "    \n",
    "\n",
    "    Omega_sur_inv = sp.linalg.inv(Omega_sur)\n",
    "    Omega_inv = np.kron(Omega_sur_inv, np.eye(time_length  * no_state,time_length  * no_state))\n",
    "    beta , y_3_hat , var , var_beta = OLS(X_gls,y_stack,Omega_inv)\n",
    "    eta = np.zeros((5,4))\n",
    "    eta[0,3] = -1\n",
    "    eta[1,3] = 1\n",
    "    eta[1,0] = -1\n",
    "    eta[2,0] = 1\n",
    "    eta[2,1] = -1\n",
    "    eta[3,1] = 1\n",
    "    eta[3,2] = -1\n",
    "    eta[4,2] = 1\n",
    "    std_eta = np.sqrt(np.diag(eta @ var_beta @ eta.T))\n",
    "    eta = eta @ beta\n",
    "    eta[0] = eta[0] +1\n",
    "    return eta,std_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_horizon_gls4(18,26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_horizon_gls4(0,45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_horizon_gls4(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_length = 30\n",
    "time_horizon_gls4(0,45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_period_grid = 45 - time_length\n",
    "smoothing_grid = np.ones((5,start_period_grid))\n",
    "time_grid = np.ones((start_period_grid,1))\n",
    "for i in range(start_period_grid):\n",
    "    time_grid[i,0] = i + 1970 + int(time_length/2)\n",
    "    coeff,var = time_horizon_gls4(i,time_length)\n",
    "    smoothing_grid[:,i] = coeff[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = smoothing_grid\n",
    "data_shape = np.shape(smoothing_grid)\n",
    "\n",
    "# Take negative and positive data apart and cumulate\n",
    "def get_cumulated_array(data, **kwargs):\n",
    "    cum = data.clip(**kwargs)\n",
    "    cum = np.cumsum(cum, axis=0)\n",
    "    d = np.zeros(np.shape(data))\n",
    "    d[1:] = cum[:-1]\n",
    "    return d  \n",
    "\n",
    "cumulated_data = get_cumulated_array(data, min=0)\n",
    "cumulated_data_neg = get_cumulated_array(data, max=0)\n",
    "\n",
    "# Re-merge negative and positive data.\n",
    "row_mask = (data<0)\n",
    "cumulated_data[row_mask] = cumulated_data_neg[row_mask]\n",
    "data_stack = cumulated_data\n",
    "\n",
    "cols = [\"m\",\"b\", \"g\", \"r\", \"c\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111)\n",
    "labels = (\"Migration\",\"Capital\",\"Federal\",\"Credit\",\"Unsmoothed\")\n",
    "for i in np.arange(0, data_shape[0]):\n",
    "    ax.bar(time_grid[:,0], data[i], label=labels[i], bottom=data_stack[i], color=cols[i],)\n",
    "ax.legend(loc=\"best\",fontsize=10)\n",
    "fig.savefig('region.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_horizon_gls5(start_period,time_length):\n",
    "    def OLS(X,y,Omega_inv = np.eye(time_length  * no_state,time_length  * no_state)):\n",
    "        beta = np.linalg.solve(X.T @ Omega_inv @  X, X.T @ Omega_inv  @ y)\n",
    "        y_hat = X @ beta\n",
    "        var = np.sum((y_hat - y)**2)/(time_length  * no_state - 1 )\n",
    "        var_beta = var *  np.linalg.inv(X.T @ X)\n",
    "        return beta , y_hat , var , var_beta\n",
    "    time_fixed_effects = np.kron(np.eye(time_length,time_length),np.ones((no_state,1)))\n",
    "    shape_vec = pe_cons[no_state:].shape[0]\n",
    "    d_pe_cons = np.reshape(pe_cons[no_state:] - pe_cons[:-no_state],(shape_vec,1))\n",
    "    d_pe_gsp = np.reshape(pe_gsp[no_state:] - pe_gsp[:-no_state],(shape_vec,1))\n",
    "    d_pe_gsp_star = np.reshape(pe_gsp_star[no_state:] - pe_gsp_star[:-no_state],(shape_vec,1))\n",
    "    d_pe_income = np.reshape(pe_income[no_state:] - pe_income[:-no_state],(shape_vec,1))\n",
    "    d_pe_disp_income = np.reshape(pe_disp_income[no_state:] - pe_disp_income[:-no_state],(shape_vec,1))\n",
    "    d_mu_gr = np.reshape(mu_gr[no_state:] - mu_gr[:-no_state],(shape_vec,1))\n",
    "    \n",
    "    d_pe_cons = demean(d_pe_cons)\n",
    "    d_pe_gsp = demean(d_pe_gsp)\n",
    "    d_pe_gsp_star = demean(d_pe_gsp_star)\n",
    "    d_pe_income = demean(d_pe_income)\n",
    "    d_pe_disp_income = demean(d_pe_disp_income)\n",
    "    d_mu_gr = demean(d_mu_gr)\n",
    "    \n",
    "    y_1 =  d_pe_income[(start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    X = d_pe_gsp_star[(start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,X_mean,var,var_beta = OLS(time_fixed_effects,X)\n",
    "    X = X - X_mean\n",
    "    X = demean(X)\n",
    "    \n",
    "    beta_mean,y_1_mean,var,var_beta = OLS(time_fixed_effects,y_1)\n",
    "    y_1 = y_1 - y_1_mean\n",
    "    y_1 = demean(y_1)\n",
    "    beta , y_1_hat , var , var_beta = OLS(X,y_1)\n",
    "    res_1 = y_1_hat - y_1\n",
    "    w_1 = res_1**2\n",
    "    w_1_diag = w_1.copy()\n",
    "    for i in range(no_state):\n",
    "        w_1_diag[i::no_state] =  np.sqrt(1/(w_1[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_1_diag)\n",
    "    X_1 = Omega_inv @ X\n",
    "    y_1g = Omega_inv @ y_1\n",
    "    res_1g = Omega_inv @ res_1\n",
    "    #Federal redistribution\n",
    "    y_2 =  d_pe_disp_income[\n",
    "       (start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,y_2_mean,var,var_beta = OLS(time_fixed_effects,y_2)\n",
    "    y_2 = y_2 - y_2_mean\n",
    "    y_2 = demean(y_2)\n",
    "    beta , y_2_hat , var , var_beta = OLS(X,y_2)\n",
    "    res_2 = y_2_hat - y_2\n",
    "    w_2 = res_2**2\n",
    "    w_2_diag = w_2.copy()\n",
    "    for i in range(no_state):\n",
    "        w_2_diag[i::no_state] =  np.sqrt(1/(w_2[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_2_diag)\n",
    "    X_2 = Omega_inv @ X\n",
    "    y_2g = Omega_inv @ y_2\n",
    "    res_2g = Omega_inv @ res_2\n",
    "    #Credit market\n",
    "    y_3 =  d_pe_cons[\n",
    "        (start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,y_3_mean,var,var_beta = OLS(time_fixed_effects,y_3)\n",
    "    y_3 = y_3 - y_3_mean\n",
    "    y_3 = demean(y_3)\n",
    "    beta , y_3_hat , var , var_beta = OLS(X,y_3)\n",
    "    res_3 = y_3_hat - y_3\n",
    "    w_3 = res_3**2\n",
    "    w_3_diag = w_3.copy()\n",
    "    for i in range(no_state):\n",
    "        w_3_diag[i::no_state] =  np.sqrt(1/(w_3[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_3_diag)\n",
    "    X_3 = Omega_inv @ X\n",
    "    y_3g = Omega_inv @ y_3\n",
    "    res_3g = Omega_inv @ res_3\n",
    "    # Migration\n",
    "    y_4 =  d_mu_gr[\n",
    "        (start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,y_4_mean,var,var_beta = OLS(time_fixed_effects,y_4)\n",
    "    y_4 = y_4 - y_4_mean\n",
    "    y_4 = demean(y_4)\n",
    "    beta , y_4_hat , var , var_beta = OLS(X,y_4)\n",
    "    res_4 = y_4_hat - y_4\n",
    "    w_4 = res_4**2\n",
    "    w_4_diag = w_4.copy()\n",
    "    for i in range(no_state):\n",
    "        w_4_diag[i::no_state] =  np.sqrt(1/(w_4[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_4_diag)\n",
    "    X_4 = Omega_inv @ X\n",
    "    y_4g = Omega_inv @ y_4\n",
    "    res_4g = Omega_inv @ res_4\n",
    "    # GDP_star\n",
    "    y_5 =  d_pe_gsp[\n",
    "        (start_period * no_state):((time_length + start_period)  * no_state)]\n",
    "    beta_mean,y_5_mean,var,var_beta = OLS(time_fixed_effects,y_5)\n",
    "    y_5 = y_5 - y_5_mean\n",
    "    y_5 = demean(y_5)\n",
    "    beta , y_5_hat , var , var_beta = OLS(X,y_5)\n",
    "    res_5 = y_5_hat - y_5\n",
    "    w_5 = res_5**2\n",
    "    w_5_diag = w_5.copy()\n",
    "    for i in range(no_state):\n",
    "        w_5_diag[i::no_state] =  np.sqrt(1/(w_5[i::no_state].mean()))\n",
    "    Omega_inv = np.diagflat(w_5_diag)\n",
    "    X_5 = Omega_inv @ X\n",
    "    y_5g = Omega_inv @ y_5\n",
    "    res_5g = Omega_inv @ res_5\n",
    "    #FGLS\n",
    "    X_gls = sp.linalg.block_diag(X_1,X_2,X_3,X_5)\n",
    "    y_stack = np.concatenate((np.concatenate((np.concatenate((y_1g,y_2g),axis =0),y_3g),axis =0),y_5g),axis =0)\n",
    "    Omega_sur = np.ones((4,4))\n",
    "    Omega_sur[0,1] = np.mean(res_1g * res_2g) \n",
    "    Omega_sur[0,2] = np.mean(res_1g * res_3g)\n",
    "    Omega_sur[0,3] = np.mean(res_1g * res_5g)\n",
    "    Omega_sur[3,0] = Omega_sur[0,3]\n",
    "    Omega_sur[2,0] = Omega_sur[0,2]\n",
    "    Omega_sur[1,0] = Omega_sur[0,1]\n",
    "    \n",
    "    \n",
    "    Omega_sur[1,2] = np.mean(res_3g * res_2g) \n",
    "    Omega_sur[1,3] = np.mean(res_5g * res_2g) \n",
    "\n",
    "    Omega_sur[2,1] = Omega_sur[1,2]\n",
    "    Omega_sur[3,1] = Omega_sur[1,3]\n",
    "\n",
    "    \n",
    "    Omega_sur[2,3] = np.mean(res_5g * res_3g) \n",
    "\n",
    "    Omega_sur[3,2] = Omega_sur[2,3]\n",
    "\n",
    "    \n",
    "\n",
    "    Omega_sur_inv = sp.linalg.inv(Omega_sur)\n",
    "    Omega_inv = np.kron(Omega_sur_inv, np.eye(time_length  * no_state,time_length  * no_state))\n",
    "    beta , y_3_hat , var , var_beta = OLS(X_gls,y_stack,Omega_inv)\n",
    "    eta = np.zeros((4,4))\n",
    "    eta[0,3] = -1\n",
    "    eta[1,3] = 1\n",
    "    eta[1,0] = -1\n",
    "    eta[2,0] = 1\n",
    "    eta[2,1] = -1\n",
    "    eta[3,1] = 1\n",
    "    eta[3,2] = 0\n",
    "    #eta[4,2] = 1\n",
    "    std_eta = np.sqrt(np.diag(eta @ var_beta @ eta.T))\n",
    "    eta = eta @ beta\n",
    "    eta[0] = eta[0] +1\n",
    "    return eta,std_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_horizon_gls5(19,26)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smoothing_grid = np.array([ 0.04482439,  0.08561039,  0.0319404 ,\n",
    "  0.46531857,  0.1549013,   0.0210731 ,\n",
    "  0.12940698 , 0.19835615  ,0.01708878,\n",
    "  0.15337855,  0.13644383 , 0.05553481,\n",
    "  0.2070715 ,  0.42468834  ,0.87436292])\n",
    "smoothing_grid = smoothing_grid.reshape(5,3)\n",
    "vertical_grid = ['US States','US Regions','EU members']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#smoothing_grid[:,0] = time_horizon_gls4(19,26)[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#smoothing_grid[:,1] = time_horizon_gls4(19,26)[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#smoothing_grid[:,2] = time_horizon_gls4(18,25)[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = smoothing_grid\n",
    "data_shape = np.shape(smoothing_grid)\n",
    "\n",
    "# Take negative and positive data apart and cumulate\n",
    "def get_cumulated_array(data, **kwargs):\n",
    "    cum = data.clip(**kwargs)\n",
    "    cum = np.cumsum(cum, axis=0)\n",
    "    d = np.zeros(np.shape(data))\n",
    "    d[1:] = cum[:-1]\n",
    "    return d  \n",
    "\n",
    "cumulated_data = get_cumulated_array(data, min=0)\n",
    "cumulated_data_neg = get_cumulated_array(data, max=0)\n",
    "\n",
    "# Re-merge negative and positive data.\n",
    "row_mask = (data<0)\n",
    "cumulated_data[row_mask] = cumulated_data_neg[row_mask]\n",
    "data_stack = cumulated_data\n",
    "\n",
    "cols = [\"purple\",\"gray\", \"g\", \"y\", \"c\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111)\n",
    "labels = (\"Migration\",\"Capital\",\"Federal\",\"Credit\",\"Unsmoothed\")\n",
    "const =np.zeros((3,1))\n",
    "for i in np.arange(0, data_shape[0]):\n",
    "    rects1 = ax.bar(np.linspace(1,3,3), data[i], label=labels[i], bottom=data_stack[i], color=cols[i],)\n",
    "    \n",
    "    for j in range(3):\n",
    "        h1 = rects1[j].get_height()\n",
    "        x = h1-0.04-0.02*i +1.01*const[j]\n",
    "        text1 = '' + str(round(100 * h1,1)) + ' %'\n",
    "        if j == 2:\n",
    "            if (i ==4):\n",
    "                x = h1-0.4-0.02*i +1.01*const[j]\n",
    "                plt.text(rects1[j].get_x() + rects1[j].get_width() / 2.,  x, text1 , ha=\"center\",  color=\"white\", fontsize=24, fontweight=\"bold\")\n",
    "            elif(i == 3):\n",
    "                x = h1-0.06+0.03*i +0.2*const[j]\n",
    "                plt.text(rects1[j].get_x() + rects1[j].get_width() / 2. + 0.47,  x, text1 , ha=\"center\",  color=\"black\", fontsize=14, fontweight=\"bold\")\n",
    "            else:\n",
    "                x = h1-0.03+0.03*i +0.2*const[j]\n",
    "                plt.text(rects1[j].get_x() + rects1[j].get_width() / 2. + 0.47,  x, text1 , ha=\"center\",  color=\"black\", fontsize=14, fontweight=\"bold\")\n",
    "        else:\n",
    "            plt.text(rects1[j].get_x() + rects1[j].get_width() / 2.,  x, text1 , ha=\"center\",  color=\"white\", fontsize=24, fontweight=\"bold\")\n",
    "        \n",
    "        const[j] =  h1 + const[j]\n",
    "plt.xticks(np.linspace(1,3,3), vertical_grid, fontweight='bold',fontsize=24)\n",
    "\n",
    "ax.legend(loc=\"best\",fontsize=24)\n",
    "fig.savefig('comparison.png', dpi=100)\n",
    "tikz_save('comparison.tex', figureheight='\\\\figureheight',\n",
    "    figurewidth='\\\\figurewidth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_tikz_code?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smoothing_grid = np.array([ 0.08561039, 0.0864, \n",
    "   0.1549013, 0.1548 ,\n",
    "   0.19835615  , 0.1963,\n",
    "   0.42468834 + 0.13644383 , 0.5625])\n",
    "smoothing_grid = smoothing_grid.reshape(4,2)\n",
    "vertical_grid = ['US Regions','Baseline']#,'Low migration','Costly adjustment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = smoothing_grid\n",
    "data_shape = np.shape(smoothing_grid)\n",
    "\n",
    "# Take negative and positive data apart and cumulate\n",
    "def get_cumulated_array(data, **kwargs):\n",
    "    cum = data.clip(**kwargs)\n",
    "    cum = np.cumsum(cum, axis=0)\n",
    "    d = np.zeros(np.shape(data))\n",
    "    d[1:] = cum[:-1]\n",
    "    return d  \n",
    "\n",
    "cumulated_data = get_cumulated_array(data, min=0)\n",
    "cumulated_data_neg = get_cumulated_array(data, max=0)\n",
    "\n",
    "# Re-merge negative and positive data.\n",
    "row_mask = (data<0)\n",
    "cumulated_data[row_mask] = cumulated_data_neg[row_mask]\n",
    "data_stack = cumulated_data\n",
    "\n",
    "cols = [\"purple\",\"gray\", \"g\", \"c\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111)\n",
    "labels = (\"Migration\",\"Capital\",\"Federal\",\"Unsmoothed\")\n",
    "const =np.zeros((smoothing_grid.shape[1],1))\n",
    "for i in np.arange(0, data_shape[0]):\n",
    "    rects1 = ax.bar(np.linspace(1,smoothing_grid.shape[1],smoothing_grid.shape[1]), data[i], label=labels[i], bottom=data_stack[i], color=cols[i],)\n",
    "    \n",
    "    for j in range(smoothing_grid.shape[1]):\n",
    "        h1 = rects1[j].get_height()\n",
    "        x = h1-0.04-0.02*i +1.01*const[j]\n",
    "        text1 = '' + str(round(100 * h1,1)) + ' %'\n",
    "        plt.text(rects1[j].get_x() + rects1[j].get_width() / 2.,  x, text1 , ha=\"center\",  color=\"white\", fontsize=24, fontweight=\"bold\")\n",
    "        \n",
    "        const[j] =  h1 + const[j]\n",
    "plt.xticks(np.linspace(1,smoothing_grid.shape[1],smoothing_grid.shape[1]), vertical_grid, fontweight='bold',fontsize=24)\n",
    "\n",
    "ax.legend(loc=\"center right\",fontsize=24)\n",
    "fig.savefig('comparison2.png', dpi=100)\n",
    "tikz_save('comparison2.tikz', figureheight='\\\\figureheight',\n",
    "    figurewidth='\\\\figurewidth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smoothing_grid = np.array([ 0.0864, 0.0808,\n",
    "    0.1548 ,0.1711,\n",
    "    0.1963,0,\n",
    "    0.5625,0.7482])\n",
    "smoothing_grid = smoothing_grid.reshape(4,2)\n",
    "vertical_grid = ['Baseline','No Federal redistribution']#,'Low migration','Costly adjustment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = smoothing_grid\n",
    "data_shape = np.shape(smoothing_grid)\n",
    "\n",
    "# Take negative and positive data apart and cumulate\n",
    "def get_cumulated_array(data, **kwargs):\n",
    "    cum = data.clip(**kwargs)\n",
    "    cum = np.cumsum(cum, axis=0)\n",
    "    d = np.zeros(np.shape(data))\n",
    "    d[1:] = cum[:-1]\n",
    "    return d  \n",
    "\n",
    "cumulated_data = get_cumulated_array(data, min=0)\n",
    "cumulated_data_neg = get_cumulated_array(data, max=0)\n",
    "\n",
    "# Re-merge negative and positive data.\n",
    "row_mask = (data<0)\n",
    "cumulated_data[row_mask] = cumulated_data_neg[row_mask]\n",
    "data_stack = cumulated_data\n",
    "\n",
    "cols = [\"purple\",\"gray\", \"g\", \"c\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111)\n",
    "labels = (\"Migration\",\"Capital\",\"Federal\",\"Unsmoothed\")\n",
    "const =np.zeros((smoothing_grid.shape[1],1))\n",
    "for i in np.arange(0, data_shape[0]):\n",
    "    rects1 = ax.bar(np.linspace(1,smoothing_grid.shape[1],smoothing_grid.shape[1]), data[i], label=labels[i], bottom=data_stack[i], color=cols[i],)\n",
    "    \n",
    "    for j in range(smoothing_grid.shape[1]):\n",
    "        h1 = rects1[j].get_height()\n",
    "        if h1 != 0:\n",
    "            x = h1-0.04-0.02*i +1.01*const[j]\n",
    "            text1 = '' + str(round(100 * h1,1)) + ' %'\n",
    "            plt.text(rects1[j].get_x() + rects1[j].get_width() / 2.,  x, text1 , ha=\"center\",  color=\"white\", fontsize=24, fontweight=\"bold\")\n",
    "\n",
    "            const[j] =  h1 + const[j]\n",
    "plt.xticks(np.linspace(1,smoothing_grid.shape[1],smoothing_grid.shape[1]), vertical_grid, fontweight='bold',fontsize=24)\n",
    "\n",
    "ax.legend(loc=\"center right\",fontsize=24)\n",
    "fig.savefig('comparison_no_federal.png', dpi=100)\n",
    "tikz_save('comparison2.tikz', figureheight='\\\\figureheight',\n",
    "    figurewidth='\\\\figurewidth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smoothing_grid = np.array([ 0.0864, 0.0190,\n",
    "    0.1548 ,0.1247,\n",
    "    0.1963,0.2124,\n",
    "    0.5625,0.6439])\n",
    "smoothing_grid = smoothing_grid.reshape(4,2)\n",
    "vertical_grid = ['Baseline','Low migration']#,'Low migration','Costly adjustment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = smoothing_grid\n",
    "data_shape = np.shape(smoothing_grid)\n",
    "\n",
    "# Take negative and positive data apart and cumulate\n",
    "def get_cumulated_array(data, **kwargs):\n",
    "    cum = data.clip(**kwargs)\n",
    "    cum = np.cumsum(cum, axis=0)\n",
    "    d = np.zeros(np.shape(data))\n",
    "    d[1:] = cum[:-1]\n",
    "    return d  \n",
    "\n",
    "cumulated_data = get_cumulated_array(data, min=0)\n",
    "cumulated_data_neg = get_cumulated_array(data, max=0)\n",
    "\n",
    "# Re-merge negative and positive data.\n",
    "row_mask = (data<0)\n",
    "cumulated_data[row_mask] = cumulated_data_neg[row_mask]\n",
    "data_stack = cumulated_data\n",
    "\n",
    "cols = [\"purple\",\"gray\", \"g\", \"c\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111)\n",
    "labels = (\"Migration\",\"Capital\",\"Federal\",\"Unsmoothed\")\n",
    "const =np.zeros((smoothing_grid.shape[1],1))\n",
    "for i in np.arange(0, data_shape[0]):\n",
    "    rects1 = ax.bar(np.linspace(1,smoothing_grid.shape[1],smoothing_grid.shape[1]), data[i], label=labels[i], bottom=data_stack[i], color=cols[i],)\n",
    "    \n",
    "    for j in range(smoothing_grid.shape[1]):\n",
    "        h1 = rects1[j].get_height()\n",
    "        if h1 > 0.02:\n",
    "            x = h1-0.04-0.02*i +1.01*const[j]\n",
    "            text1 = '' + str(round(100 * h1,1)) + ' %'\n",
    "            plt.text(rects1[j].get_x() + rects1[j].get_width() / 2.,  x, text1 , ha=\"center\",  color=\"white\", fontsize=24, fontweight=\"bold\")\n",
    "\n",
    "            const[j] =  h1 + const[j]\n",
    "        else:\n",
    "            x = h1-0.01-0.02*i +1.01*const[j]\n",
    "            text1 = '' + str(round(100 * h1,2)) + ' %'\n",
    "            plt.text(rects1[j].get_x() + rects1[j].get_width() / 2.,  x, text1 , ha=\"center\",  color=\"white\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "            const[j] =  h1 + const[j]\n",
    "plt.xticks(np.linspace(1,smoothing_grid.shape[1],smoothing_grid.shape[1]), vertical_grid, fontweight='bold',fontsize=24)\n",
    "\n",
    "ax.legend(loc=\"center right\",fontsize=24)\n",
    "fig.savefig('comparison_low_migr.png', dpi=100)\n",
    "tikz_save('comparison2.tikz', figureheight='\\\\figureheight',\n",
    "    figurewidth='\\\\figurewidth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smoothing_grid = np.array([ 0.0864, 0.0795,\n",
    "    0.1548 ,0.0869,\n",
    "    0.1963,0.1949,\n",
    "    0.5625,0.6388])\n",
    "smoothing_grid = smoothing_grid.reshape(4,2)\n",
    "vertical_grid = ['Baseline','Costly adjustment']#,'Low migration','Costly adjustment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = smoothing_grid\n",
    "data_shape = np.shape(smoothing_grid)\n",
    "\n",
    "# Take negative and positive data apart and cumulate\n",
    "def get_cumulated_array(data, **kwargs):\n",
    "    cum = data.clip(**kwargs)\n",
    "    cum = np.cumsum(cum, axis=0)\n",
    "    d = np.zeros(np.shape(data))\n",
    "    d[1:] = cum[:-1]\n",
    "    return d  \n",
    "\n",
    "cumulated_data = get_cumulated_array(data, min=0)\n",
    "cumulated_data_neg = get_cumulated_array(data, max=0)\n",
    "\n",
    "# Re-merge negative and positive data.\n",
    "row_mask = (data<0)\n",
    "cumulated_data[row_mask] = cumulated_data_neg[row_mask]\n",
    "data_stack = cumulated_data\n",
    "\n",
    "cols = [\"purple\",\"gray\", \"g\", \"c\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111)\n",
    "labels = (\"Migration\",\"Capital\",\"Federal\",\"Unsmoothed\")\n",
    "const =np.zeros((smoothing_grid.shape[1],1))\n",
    "for i in np.arange(0, data_shape[0]):\n",
    "    rects1 = ax.bar(np.linspace(1,smoothing_grid.shape[1],smoothing_grid.shape[1]), data[i], label=labels[i], bottom=data_stack[i], color=cols[i],)\n",
    "    \n",
    "    for j in range(smoothing_grid.shape[1]):\n",
    "        h1 = rects1[j].get_height()\n",
    "        if h1 > 0.02:\n",
    "            x = h1-0.04-0.02*i +1.01*const[j]\n",
    "            text1 = '' + str(round(100 * h1,1)) + ' %'\n",
    "            plt.text(rects1[j].get_x() + rects1[j].get_width() / 2.,  x, text1 , ha=\"center\",  color=\"white\", fontsize=24, fontweight=\"bold\")\n",
    "\n",
    "            const[j] =  h1 + const[j]\n",
    "        else:\n",
    "            x = h1-0.01-0.02*i +1.01*const[j]\n",
    "            text1 = '' + str(round(100 * h1,1)) + ' %'\n",
    "            plt.text(rects1[j].get_x() + rects1[j].get_width() / 2.,  x, text1 , ha=\"center\",  color=\"white\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "            const[j] =  h1 + const[j]\n",
    "plt.xticks(np.linspace(1,smoothing_grid.shape[1],smoothing_grid.shape[1]), vertical_grid, fontweight='bold',fontsize=24)\n",
    "\n",
    "ax.legend(loc=\"center right\",fontsize=24)\n",
    "fig.savefig('comparison_high_adj.png', dpi=100)\n",
    "tikz_save('comparison2.tikz', figureheight='\\\\figureheight',\n",
    "    figurewidth='\\\\figurewidth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smoothing_grid = np.array([  0, 0,0.0006,0.0006,\n",
    "    0.0655 ,0.0747,0.0686,0.3526,\n",
    "    0.0,0.2537,0,0,\n",
    "    0.9347,0.6718,0.9318,0.6468])\n",
    "smoothing_grid = smoothing_grid.reshape(4,4)\n",
    "vertical_grid = ['Baseline','Federal government','Higher migration','Better adjustment']#,'Low migration','Costly adjustment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = smoothing_grid\n",
    "data_shape = np.shape(smoothing_grid)\n",
    "\n",
    "# Take negative and positive data apart and cumulate\n",
    "def get_cumulated_array(data, **kwargs):\n",
    "    cum = data.clip(**kwargs)\n",
    "    cum = np.cumsum(cum, axis=0)\n",
    "    d = np.zeros(np.shape(data))\n",
    "    d[1:] = cum[:-1]\n",
    "    return d  \n",
    "\n",
    "cumulated_data = get_cumulated_array(data, min=0)\n",
    "cumulated_data_neg = get_cumulated_array(data, max=0)\n",
    "\n",
    "# Re-merge negative and positive data.\n",
    "row_mask = (data<0)\n",
    "cumulated_data[row_mask] = cumulated_data_neg[row_mask]\n",
    "data_stack = cumulated_data\n",
    "\n",
    "cols = [\"purple\",\"gray\", \"g\", \"c\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111)\n",
    "labels = (\"Migration\",\"Capital\",\"Federal\",\"Unsmoothed\")\n",
    "const =np.zeros((smoothing_grid.shape[1],1))\n",
    "for i in np.arange(0, data_shape[0]):\n",
    "    rects1 = ax.bar(np.linspace(1,smoothing_grid.shape[1],smoothing_grid.shape[1]), data[i], label=labels[i], bottom=data_stack[i], color=cols[i],)\n",
    "    \n",
    "    for j in range(smoothing_grid.shape[1]):\n",
    "        h1 = rects1[j].get_height()\n",
    "        if h1 > 0.02:\n",
    "            x = h1-0.04-0.02*i +1.01*const[j]\n",
    "            text1 = '' + str(round(100 * h1,1)) + ' %'\n",
    "            plt.text(rects1[j].get_x() + rects1[j].get_width() / 2.,  x, text1 , ha=\"center\",  color=\"white\", fontsize=24, fontweight=\"bold\")\n",
    "\n",
    "            const[j] =  h1 + const[j]\n",
    "        else:\n",
    "            x = h1-0.01-0.02*i +1.01*const[j]\n",
    "            text1 = '' + str(round(100 * h1,1)) + ' %'\n",
    "            #plt.text(rects1[j].get_x() + rects1[j].get_width() / 2.,  x, text1 , ha=\"center\",  color=\"white\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "            const[j] =  h1 + const[j]\n",
    "plt.xticks(np.linspace(1,smoothing_grid.shape[1],smoothing_grid.shape[1]), vertical_grid, fontweight='bold',fontsize=16)\n",
    "\n",
    "ax.legend(loc=\"center right\",fontsize=24)\n",
    "fig.savefig('comparison3.png', dpi=100)\n",
    "#tikz_save('comparison3.tikz', figureheight='\\\\figureheight',\n",
    "#    figurewidth='\\\\figurewidth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
